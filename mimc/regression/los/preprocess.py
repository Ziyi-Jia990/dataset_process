import os
import pandas as pd
import numpy as np
import torch
from sklearn.preprocessing import StandardScaler
from PIL import Image
from tqdm import tqdm

# ç¡®ä¿ PIL ç‰ˆæœ¬å…¼å®¹
try:
    LANCZOS_RESAMPLE = Image.Resampling.LANCZOS
except AttributeError:
    LANCZOS_RESAMPLE = Image.LANCZOS

# --- 1. å®šä¹‰å¸¸é‡å’Œè·¯å¾„ ---
# [é…ç½®] è¾“å…¥è·¯å¾„
CSV_FILE = "/mnt/hdd/jiazy/mimic/regression/los/los_dataset_100k.csv"
IMAGE_ROOT = "/mnt/hdd/jiazy/mimic/image"

# [é…ç½®] è¾“å‡ºç›®å½•
OUTPUT_DIR = "/mnt/hdd/jiazy/mimic/regression/los/features"

if not os.path.exists(OUTPUT_DIR):
    os.makedirs(OUTPUT_DIR)

# --- 2. å®šä¹‰ç‰¹å¾åˆ— ---
# è¿ç»­ç‰¹å¾ (14ä¸ª)
CONTINUOUS_COLS = [
    'age', 'Temperature', 'HeartRate', 'RespRate', 'SpO2', 
    'SysBP', 'DiaBP', 'WBC', 'Hemoglobin', 'Platelet', 
    'Glucose', 'Creatinine', 'Sodium', 'Potassium'
]

# ç±»åˆ«ç‰¹å¾ (4ä¸ª)
CATEGORICAL_COLS = [
    'gender', 'ViewPosition', 'admission_type', 'admission_location'
]

# æ ‡ç­¾åˆ—
LABEL_COL = 'target_los_days'

# éœ€è¦ä½¿ç”¨çš„åˆ— (ç”¨äºä»CSVä¸­æå–)
USE_COLS = ['split', 'image_path', LABEL_COL] + CONTINUOUS_COLS + CATEGORICAL_COLS

# --- 3. å›¾åƒå¤„ç†è¾…åŠ©å‡½æ•° ---
def process_and_save_image(rel_path):
    """
    åŠ è½½å›¾åƒ, resize åˆ° 224x224 (ä¸è£å‰ª), å¹¶å¦å­˜ä¸º .npy æ–‡ä»¶ã€‚
    å‚æ•°:
        rel_path: CSVä¸­ image_path åˆ—çš„å€¼ (å¦‚ p10/p100001/s501/view1.jpg)
    """
    if pd.isna(rel_path):
        return None

    # æ‹¼æ¥å®Œæ•´è·¯å¾„
    full_img_path = os.path.join(IMAGE_ROOT, rel_path)
    
    # æ„é€  .npy ä¿å­˜è·¯å¾„ (ç›´æ¥ä¿å­˜åœ¨åŸå›¾ç‰‡åŒç›®å½•ä¸‹ï¼Œæ–‡ä»¶ååç¼€æ”¹ä¸º .npy)
    # æ³¨æ„ï¼šè¿™é‡Œå‡è®¾ä½ æœ‰å¯¹åŸå§‹å›¾ç‰‡ç›®å½•çš„å†™å…¥æƒé™
    npy_path = full_img_path.replace(".jpg", ".npy").replace(".jpeg", ".npy").replace(".png", ".npy")

    # å¦‚æœ .npy å·²ç»å­˜åœ¨ï¼Œç›´æ¥è¿”å›è·¯å¾„
    if os.path.exists(npy_path):
        return npy_path

    if not os.path.exists(full_img_path):
        # å›¾ç‰‡ç‰©ç†æ–‡ä»¶ä¸å­˜åœ¨
        return None 

    try:
        # æ‰“å¼€å›¾åƒ
        img = Image.open(full_img_path)
        
        # Resize åˆ° 224x224 (ç›´æ¥ç¼©æ”¾ï¼Œä¸ä¿æŒæ¯”ä¾‹è£å‰ªï¼Œç¬¦åˆä½ çš„ "ç¼©æ”¾è€Œä¸æ˜¯è£å‰ª" è¦æ±‚)
        img_resized = img.resize((224, 224), resample=LANCZOS_RESAMPLE)
        
        # è½¬æ¢ä¸º RGB (CTé€šå¸¸æ˜¯å•é€šé“ç°åº¦å›¾ï¼Œé€‚é…ä»£ç æ¡†æ¶é€šå¸¸éœ€è¦3é€šé“)
        if img_resized.mode != 'RGB':
            img_resized = img_resized.convert('RGB')
            
        np_img = np.array(img_resized)
        
        # ä¿å­˜ä¸º .npy
        np.save(npy_path, np_img)
        
        return npy_path
    except Exception as e:
        print(f"å¤„ç†å›¾åƒå¤±è´¥ {full_img_path}: {e}")
        return None

def preprocess_mimic_flow():
    """
    é¢„å¤„ç† MIMIC-CXR æ•°æ®é›†
    """
    print("å¼€å§‹æ‰§è¡Œ MIMIC-CXR æ•°æ®é›†é¢„å¤„ç†æµç¨‹...")
    print(f"è¾“å‡ºç›®å½•: {OUTPUT_DIR}")
    
    # --- 1. ğŸ“– åŠ è½½æ•°æ® ---
    print(f"æ­£åœ¨åŠ è½½å…ƒæ•°æ® {CSV_FILE}...")
    df = pd.read_csv(CSV_FILE, usecols=USE_COLS)
    print(f"åŸå§‹æ•°æ®æ¡æ•°: {len(df)}")

    # --- 2. ğŸ§¹ åŸºç¡€æ•°æ®æ¸…æ´— ---
    # è¿‡æ»¤æ‰æ ‡ç­¾ç¼ºå¤±çš„è¡Œ
    df = df.dropna(subset=[LABEL_COL])
    print(f"è¿‡æ»¤æ— æ ‡ç­¾æ•°æ®åæ¡æ•°: {len(df)}")
    
    # --- 3. ğŸ·ï¸ ç±»åˆ«ç‰¹å¾ç¼–ç  ---
    print("æ­£åœ¨è¿›è¡Œç±»åˆ«ç‰¹å¾ç¼–ç ...")
    cat_dims = [] # è®°å½•æ¯ä¸ªç‰¹å¾çš„ç±»åˆ«æ•°
    for col in CATEGORICAL_COLS:
        # å¡«å……ç¼ºå¤±å€¼ä¸º -1 (æˆ–è€…ä½ å¯ä»¥é€‰æ‹© 'Unknown')
        df[col] = df[col].fillna(-1) 
        df[col] = df[col].astype('category')
        # è·å–ç¼–ç  (0, 1, 2...)
        df[col] = df[col].cat.codes
        # è®°å½•ç»´åº¦ (æœ€å¤§ID + 1), å¦‚æœå…¨æ˜¯-1åˆ™ç»´åº¦ä¸º0(å®é™…ä¸Šåº”è¯¥è‡³å°‘ä¸º1, è¿™é‡Œä¸ºäº†ä»£ç é²æ£’æ€§)
        num_cats = int(df[col].max() + 1)
        cat_dims.append(num_cats if num_cats > 0 else 1)
    
    print(f"ç±»åˆ«ç‰¹å¾ç»´åº¦: {dict(zip(CATEGORICAL_COLS, cat_dims))}")

    # --- 4. âœ‚ï¸ æ•°æ®é›†åˆ’åˆ† (åŸºäº split åˆ—) ---
    print("æ­£åœ¨åŸºäº 'split' åˆ—è¿›è¡Œæ•°æ®é›†åˆ’åˆ†...")
    # ä½ çš„ split åˆ—åŒ…å«: train, valid, test
    train_df = df[df['split'] == 'train'].copy()
    val_df = df[df['split'] == 'valid'].copy()
    test_df = df[df['split'] == 'test'].copy()

    print(f"åˆ’åˆ†ç»“æœ: è®­ç»ƒé›† {len(train_df)}, éªŒè¯é›† {len(val_df)}, æµ‹è¯•é›† {len(test_df)}")

    # --- 5. ğŸ“ è¿ç»­ç‰¹å¾æ ‡å‡†åŒ– ---
    print("æ­£åœ¨æ ‡å‡†åŒ–è¿ç»­ç‰¹å¾...")
    # æ³¨æ„ï¼šåªåœ¨è®­ç»ƒé›†ä¸Š fitï¼Œåœ¨æ‰€æœ‰é›†ä¸Š transform
    if CONTINUOUS_COLS:
        scaler = StandardScaler()
        # å¡«å……è¿ç»­ç‰¹å¾çš„ç¼ºå¤±å€¼ï¼Œé€šå¸¸ç”¨å‡å€¼å¡«å……ï¼Œè¿™é‡Œç®€å•èµ·è§å…ˆç”¨0æˆ–è®­ç»ƒé›†å‡å€¼
        # ä¸ºé˜²æ­¢æŠ¥é”™ï¼Œå…ˆå¡«0ã€‚æ›´å¥½çš„åšæ³•æ˜¯åœ¨ fit ä¹‹å‰ç”¨ train çš„å‡å€¼å¡«
        train_df[CONTINUOUS_COLS] = train_df[CONTINUOUS_COLS].fillna(0)
        val_df[CONTINUOUS_COLS] = val_df[CONTINUOUS_COLS].fillna(0)
        test_df[CONTINUOUS_COLS] = test_df[CONTINUOUS_COLS].fillna(0)

        scaler.fit(train_df[CONTINUOUS_COLS])
        
        train_df[CONTINUOUS_COLS] = scaler.transform(train_df[CONTINUOUS_COLS])
        val_df[CONTINUOUS_COLS] = scaler.transform(val_df[CONTINUOUS_COLS])
        test_df[CONTINUOUS_COLS] = scaler.transform(test_df[CONTINUOUS_COLS])

    # --- 6. ğŸ–¼ï¸ å›¾åƒå¤„ç† ---
    print("æ­£åœ¨å¤„ç†å›¾åƒ (Resize & Save .npy)...")
    # ä½¿ç”¨ tqdm æ˜¾ç¤ºè¿›åº¦
    tqdm.pandas(desc="Train Images")
    train_df['npy_path'] = train_df['image_path'].progress_apply(process_and_save_image)
    
    tqdm.pandas(desc="Val Images")
    val_df['npy_path'] = val_df['image_path'].progress_apply(process_and_save_image)
    
    tqdm.pandas(desc="Test Images")
    test_df['npy_path'] = test_df['image_path'].progress_apply(process_and_save_image)

    # è¿‡æ»¤æ‰å›¾åƒå¤„ç†å¤±è´¥çš„ (è¿”å› None çš„)
    len_before = len(train_df) + len(val_df) + len(test_df)
    train_df = train_df.dropna(subset=['npy_path'])
    val_df = val_df.dropna(subset=['npy_path'])
    test_df = test_df.dropna(subset=['npy_path'])
    len_after = len(train_df) + len(val_df) + len(test_df)
    
    if len_before != len_after:
        print(f"âš ï¸ è­¦å‘Šï¼šè¿‡æ»¤äº† {len_before - len_after} æ¡å›¾åƒä¸å­˜åœ¨æˆ–æŸåçš„æ•°æ®ã€‚")

    # --- 7. ğŸ’¾ ä¿å­˜è¾“å‡º ---
    print(f"[Final] æ­£åœ¨ä¿å­˜å¤„ç†åçš„æ–‡ä»¶åˆ° {OUTPUT_DIR} ...")

    # ä¿å­˜ tabular_lengths: å…ˆç±»åˆ«ï¼Œåè¿ç»­
    # è¿ç»­ç‰¹å¾ç»´åº¦å›ºå®šä¸º 1
    tabular_lengths = cat_dims + [1] * len(CONTINUOUS_COLS)
    torch.save(tabular_lengths, os.path.join(OUTPUT_DIR, "tabular_lengths.pt"))
    print(f"ä¿å­˜ tabular_lengths, é•¿åº¦: {len(tabular_lengths)}")
    
    for split_name, df_split in zip(["train", "val", "test"], [train_df, val_df, test_df]):
        # æ³¨æ„ï¼šè¿™é‡Œæ–‡ä»¶åç”¨ val è€Œä¸æ˜¯ validï¼Œä¸ºäº†é€‚é…åŸä»£ç çš„å‘½åä¹ æƒ¯
        
        # 1. ä¿å­˜ç‰¹å¾ CSV (æ— è¡¨å¤´ï¼Œå…ˆç±»åˆ«åè¿ç»­)
        features_path = os.path.join(OUTPUT_DIR, f"{split_name}_features.csv")
        cols_to_save = CATEGORICAL_COLS + CONTINUOUS_COLS
        df_split[cols_to_save].to_csv(features_path, index=False, header=False)
        
        # 2. ä¿å­˜æ ‡ç­¾ Tensor (Float32 ç”¨äºå›å½’)
        labels_path = os.path.join(OUTPUT_DIR, f"{split_name}_labels.pt")
        labels_tensor = torch.tensor(df_split[LABEL_COL].values, dtype=torch.float32)
        torch.save(labels_tensor, labels_path)
        
        # 3. ä¿å­˜è·¯å¾„ List
        paths_path = os.path.join(OUTPUT_DIR, f"{split_name}_paths.pt")
        npy_path_list = df_split['npy_path'].tolist()
        torch.save(npy_path_list, paths_path)

    print("âœ… MIMIC é¢„å¤„ç†å…¨éƒ¨å®Œæˆï¼")

if __name__ == "__main__":
    preprocess_mimic_flow()