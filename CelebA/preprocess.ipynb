{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5c94a136",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "å¼€å§‹æ‰§è¡Œ CelebA é¢„å¤„ç†æµç¨‹ (æ ‡ç­¾: Attractive)...\n",
      " æ­£åœ¨åŠ è½½æ ‡æ³¨æ–‡ä»¶ (list_attr_celeba.csv)...\n",
      " > æˆåŠŸåŠ è½½ 202599 æ¡è®°å½•ã€‚\n",
      " æ­£åœ¨å®šä¹‰ç‰¹å¾...\n",
      " > å·²å®šä¹‰ 0 ä¸ªè¿ç»­ç‰¹å¾ã€‚\n",
      " > å·²å®šä¹‰ 40 ä¸ªç±»åˆ«ç‰¹å¾ (åŒ…å«æ ‡ç­¾)ã€‚\n",
      " > ç›®æ ‡æ ‡ç­¾ (Label): Attractive\n",
      " æ­£åœ¨æŒ‰ 8:1:1 å¯¹ Attractive è¿›è¡Œåˆ†å±‚åˆ’åˆ†...\n",
      " > è®­ç»ƒé›†: 162079 æ ·æœ¬\n",
      " > éªŒè¯é›†: 20260 æ ·æœ¬\n",
      " > æµ‹è¯•é›†: 20260 æ ·æœ¬\n",
      " æ­£åœ¨ç­›é€‰æ’å®šç‰¹å¾...\n",
      " > å·²å°† 'Attractive' éš”ç¦»ä¸ºæ ‡ç­¾, ä¸ä½œä¸ºç‰¹å¾ã€‚\n",
      " > å‰©ä½™ 0 ä¸ªè¿ç»­ç‰¹å¾ã€‚\n",
      " > å‰©ä½™ 39 ä¸ªç±»åˆ«ç‰¹å¾ (è¡¨æ ¼å±æ€§)ã€‚\n",
      " æ­£åœ¨è½¬æ¢å›¾åƒè·¯å¾„ä¸ºç»å¯¹è·¯å¾„...\n",
      " æ­£åœ¨è¿‡æ»¤æ— æ•ˆè¡Œ (åŸºäºè·¯å¾„å’Œæ ‡ç­¾)...\n",
      " > è®­ç»ƒé›†ç§»é™¤ 0 è¡Œ\n",
      " > éªŒè¯é›†ç§»é™¤ 0 è¡Œ\n",
      " > æµ‹è¯•é›†ç§»é™¤ 0 è¡Œ\n",
      " æ­£åœ¨å¤„ç†ç¼ºå¤±å€¼/ç‰¹æ®Šç¼–ç ...\n",
      " > [CelebA ç‰¹å®š] è½¬æ¢å±æ€§ï¼šå°† -1 æ˜ å°„åˆ° 0...\n",
      " æ­£åœ¨å¤„ç†ç±»åˆ«ç‰¹å¾...\n",
      " > CelebA å±æ€§å·²ä¸º 0/1 ç¼–ç ï¼Œè·³è¿‡.cat.codesã€‚\n",
      " > æ­£åœ¨è®¡ç®—åŸºæ•° (Cardinalities)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_292101/4247396634.py:196: FutureWarning: elementwise comparison failed; returning scalar instead, but in the future will perform elementwise comparison\n",
      "  if \"MISSING\" in all_values.unique():\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " > 39 ä¸ªç±»åˆ«ç‰¹å¾çš„åŸºæ•° (éƒ¨åˆ†): [2, 2, 2, 2, 2]...\n",
      " æ­£åœ¨æ ‡å‡†åŒ–è¿ç»­ç‰¹å¾...\n",
      " > æ²¡æœ‰è¿ç»­ç‰¹å¾ï¼Œè·³è¿‡æ ‡å‡†åŒ–ã€‚\n",
      " æ­£åœ¨è½¬æ¢å›¾åƒ (JPG -> NPY, å¹¶ Resize åˆ° 224x224)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " > è½¬æ¢ è®­ç»ƒé›† å›¾åƒ: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 162079/162079 [00:11<00:00, 14014.53it/s]\n",
      " > è½¬æ¢ éªŒè¯é›† å›¾åƒ: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20260/20260 [00:00<00:00, 57746.67it/s]\n",
      " > è½¬æ¢ æµ‹è¯•é›† å›¾åƒ: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20260/20260 [00:00<00:00, 55325.04it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " > å›¾åƒè½¬æ¢å®Œæˆã€‚\n",
      "[Final] æ­£åœ¨ä¿å­˜æ‰€æœ‰å¤„ç†åçš„æ–‡ä»¶...\n",
      "------------------------------\n",
      "ğŸ‰ é¢„å¤„ç†å…¨éƒ¨å®Œæˆï¼ ğŸ‰\n",
      "è¾“å‡ºæ–‡ä»¶å·²ä¿å­˜åˆ°: ./features\n",
      " > æ ‡ç­¾ (Label): Attractive\n",
      " > è¡¨æ ¼ç‰¹å¾: 39 ä¸ªå…¶ä»–å±æ€§\n",
      " > åˆ’åˆ†æ¯”ä¾‹: 8:1:1 (åˆ†å±‚)\n",
      " > å›¾åƒå¤„ç†: å·²ç¼©æ”¾è‡³ 224x224 å¹¶è½¬ä¸º .npy\n",
      "------------------------------\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from sklearn.model_selection import train_test_split # å¯¼å…¥ train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "import warnings\n",
    "\n",
    "# ç¡®ä¿ PIL ç‰ˆæœ¬å…¼å®¹ (ç”¨äº resize)\n",
    "try:\n",
    "    # PIL 9.0.0+\n",
    "    LANCZOS_RESAMPLE = Image.Resampling.LANCZOS\n",
    "except AttributeError:\n",
    "    # Older PIL\n",
    "    LANCZOS_RESAMPLE = Image.LANCZOS\n",
    "\n",
    "# --- é…ç½®åŒºåŸŸ ---\n",
    "\n",
    "# *** 1. è·¯å¾„å·²æ ¹æ®æ‚¨çš„è¦æ±‚ä¿®æ”¹ ***\n",
    "DATA_ROOT = \"/mnt/hdd/jiazy/CelebA\"\n",
    "ATTR_FILE = os.path.join(DATA_ROOT, \"list_attr_celeba.csv\")\n",
    "BASE_IMAGE_DIR = os.path.join(DATA_ROOT, \"img_align_celeba\")\n",
    "\n",
    "# ä¿å­˜å¤„ç†åæ–‡ä»¶çš„è¾“å‡ºç›®å½•\n",
    "OUTPUT_DIR = \"./features\"\n",
    "\n",
    "# --- å…³é”®æ›´æ”¹ ---\n",
    "# ç›®æ ‡æ ‡ç­¾\n",
    "LABEL_COLUMN = 'Attractive' \n",
    "# æ–°å¢ï¼šå›¾åƒå°ºå¯¸\n",
    "IMAGE_SIZE = 224\n",
    "\n",
    "# --------------------\n",
    "\n",
    "def preprocess_celeba_flow(attr_file, base_image_dir, output_dir, label_column):\n",
    "    \"\"\"\n",
    "    æ ¹æ®ç”¨æˆ·æ–°éœ€æ±‚ (8:1:1 åˆ’åˆ†, CSVåŠ è½½, 224x224 resize) é¢„å¤„ç† CelebA\n",
    "    \"\"\"\n",
    "    print(f\"å¼€å§‹æ‰§è¡Œ CelebA é¢„å¤„ç†æµç¨‹ (æ ‡ç­¾: {label_column})...\")\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "    # --- 1. ğŸ“– åŠ è½½æ•°æ® (Step 1) ---\n",
    "    # *** 2. å·²ä¿®æ”¹ï¼šåŠ è½½ CSV è€Œä¸æ˜¯ TXTï¼Œå¹¶ä¸”ä¸å†åŠ è½½ partition æ–‡ä»¶ ***\n",
    "    print(\" æ­£åœ¨åŠ è½½æ ‡æ³¨æ–‡ä»¶ (list_attr_celeba.csv)...\")\n",
    "    try:\n",
    "        # åŠ è½½ .csv æ–‡ä»¶\n",
    "        df_master = pd.read_csv(ATTR_FILE)\n",
    "    except FileNotFoundError as e:\n",
    "        print(f\"é”™è¯¯ï¼šæ‰¾ä¸åˆ°æ–‡ä»¶ {e.filename}ã€‚\")\n",
    "        print(\"è¯·ç¡®ä¿ DATA_ROOT å’Œ ATTR_FILE è®¾ç½®æ­£ç¡®ã€‚\")\n",
    "        return\n",
    "    \n",
    "    # åŸå§‹è„šæœ¬çš„ partition å’Œ merge éƒ¨åˆ†å·²è¢«ç§»é™¤\n",
    "    print(f\" > æˆåŠŸåŠ è½½ {len(df_master)} æ¡è®°å½•ã€‚\")\n",
    "\n",
    "    # --- 2. ğŸ“ å®šä¹‰ç‰¹å¾ (Step 2) ---\n",
    "    print(\" æ­£åœ¨å®šä¹‰ç‰¹å¾...\")\n",
    "    \n",
    "    # *** 3. å·²ä¿®å¤ï¼šåˆå§‹åŒ–ä¸ºç©ºåˆ—è¡¨ ***\n",
    "    continuous_cols = []\n",
    "    \n",
    "    # ç±»åˆ«ç‰¹å¾æ˜¯ 40 ä¸ªå±æ€§\n",
    "    # æˆ‘ä»¬ä» 'image_id' ä¸­æ’é™¤\n",
    "    # (å‡è®¾ 'image_id' æ˜¯ .csv çš„ç¬¬ä¸€åˆ—)\n",
    "    categorical_cols = df_master.columns.drop(['image_id']).tolist()\n",
    "    \n",
    "    # ç¡®ä¿æˆ‘ä»¬é€‰æ‹©çš„æ ‡ç­¾åœ¨ç±»åˆ«ç‰¹å¾åˆ—è¡¨ä¸­ï¼ˆåç»­ä¼šç§»é™¤ï¼‰\n",
    "    if label_column not in categorical_cols:\n",
    "        print(f\"é”™è¯¯ï¼šæ ‡ç­¾ '{label_column}' ä¸åœ¨å±æ€§åˆ—è¡¨ä¸­ã€‚\")\n",
    "        print(f\"å¯ç”¨å±æ€§ï¼š{categorical_cols}\")\n",
    "        return\n",
    "        \n",
    "    # å®šä¹‰éç‰¹å¾åˆ—\n",
    "    # *** 4. å·²ä¿®æ”¹ï¼šç§»é™¤ 'split' ***\n",
    "    non_feature_cols = ['image_id', label_column]\n",
    "    \n",
    "    print(f\" > å·²å®šä¹‰ {len(continuous_cols)} ä¸ªè¿ç»­ç‰¹å¾ã€‚\")\n",
    "    print(f\" > å·²å®šä¹‰ {len(categorical_cols)} ä¸ªç±»åˆ«ç‰¹å¾ (åŒ…å«æ ‡ç­¾)ã€‚\")\n",
    "    print(f\" > ç›®æ ‡æ ‡ç­¾ (Label): {label_column}\")\n",
    "\n",
    "    # --- 8. ğŸ”ª æ•°æ®é›†åˆ’åˆ† (Step 8) ---\n",
    "    # *** 5. å·²å½»åº•ä¿®æ”¹ï¼šä½¿ç”¨ 8:1:1 åˆ†å±‚åˆ’åˆ†ï¼Œæ›¿æ¢åŸæœ‰çš„å›ºå®šåˆ’åˆ† ***\n",
    "    print(f\" æ­£åœ¨æŒ‰ 8:1:1 å¯¹ {label_column} è¿›è¡Œåˆ†å±‚åˆ’åˆ†...\")\n",
    "    \n",
    "    # é¦–å…ˆï¼Œåˆ†å±‚åˆ’åˆ†å‡º 80% çš„è®­ç»ƒé›†\n",
    "    train_df, temp_df = train_test_split(\n",
    "        df_master,\n",
    "        test_size=0.2,\n",
    "        stratify=df_master[label_column],\n",
    "        random_state=42\n",
    "    )\n",
    "    \n",
    "    # ç„¶åï¼Œå°†å‰©ä½™çš„ 20% å¹³åˆ†ä¸º 10% éªŒè¯é›†å’Œ 10% æµ‹è¯•é›†\n",
    "    val_df, test_df = train_test_split(\n",
    "        temp_df,\n",
    "        test_size=0.5,\n",
    "        stratify=temp_df[label_column],\n",
    "        random_state=42\n",
    "    )\n",
    "    \n",
    "    print(f\" > è®­ç»ƒé›†: {len(train_df)} æ ·æœ¬\")\n",
    "    print(f\" > éªŒè¯é›†: {len(val_df)} æ ·æœ¬\")\n",
    "    print(f\" > æµ‹è¯•é›†: {len(test_df)} æ ·æœ¬\")\n",
    "\n",
    "    # --- 3. ğŸ§¹ ç‰¹å¾ç­›é€‰ (Step 3) ---\n",
    "    # (æ­¤æ­¥éª¤é€»è¾‘åœ¨æ–°çš„åˆ’åˆ†ä¸Šä¾ç„¶æœ‰æ•ˆ)\n",
    "    print(\" æ­£åœ¨ç­›é€‰æ’å®šç‰¹å¾...\")\n",
    "    \n",
    "    final_continuous_cols = list(continuous_cols)\n",
    "    final_categorical_cols = list(categorical_cols)\n",
    "    \n",
    "    # (æ­¤éƒ¨åˆ†ä»£ç ä¸å˜ï¼Œä½†ç°åœ¨ä½œç”¨äºæ–°åˆ’åˆ†çš„æ•°æ®é›†)\n",
    "    for col in continuous_cols:\n",
    "        is_constant_train = train_df[col].nunique() <= 1\n",
    "        is_constant_test = test_df[col].nunique() <= 1\n",
    "        if is_constant_train and is_constant_test:\n",
    "            print(f\" > ç§»é™¤æ’å®šè¿ç»­ç‰¹å¾: {col}\")\n",
    "            final_continuous_cols.remove(col)\n",
    "            \n",
    "    for col in categorical_cols:\n",
    "        is_constant_train = train_df[col].nunique() <= 1\n",
    "        is_constant_test = test_df[col].nunique() <= 1\n",
    "        if is_constant_train and is_constant_test:\n",
    "            print(f\" > ç§»é™¤æ’å®šç±»åˆ«ç‰¹å¾: {col}\")\n",
    "            final_categorical_cols.remove(col)\n",
    "            \n",
    "    # ç¡®ä¿ç›®æ ‡æ ‡ç­¾(label_column)ä¸è¢«è§†ä¸ºç‰¹å¾\n",
    "    if label_column in final_categorical_cols:\n",
    "        final_categorical_cols.remove(label_column)\n",
    "        print(f\" > å·²å°† '{label_column}' éš”ç¦»ä¸ºæ ‡ç­¾, ä¸ä½œä¸ºç‰¹å¾ã€‚\")\n",
    "        \n",
    "    print(f\" > å‰©ä½™ {len(final_continuous_cols)} ä¸ªè¿ç»­ç‰¹å¾ã€‚\")\n",
    "    print(f\" > å‰©ä½™ {len(final_categorical_cols)} ä¸ªç±»åˆ«ç‰¹å¾ (è¡¨æ ¼å±æ€§)ã€‚\")\n",
    "\n",
    "    # --- 4. ğŸ—ºï¸ è·¯å¾„å¤„ç† (Step 4) ---\n",
    "    print(\" æ­£åœ¨è½¬æ¢å›¾åƒè·¯å¾„ä¸ºç»å¯¹è·¯å¾„...\")\n",
    "    \n",
    "    def create_absolute_path(image_id):\n",
    "        # å‡è®¾ image_id å·²ç»æ˜¯ '000001.jpg' è¿™æ ·çš„æ–‡ä»¶å\n",
    "        return os.path.join(base_image_dir, image_id)\n",
    "\n",
    "    train_df['absolute_image_path'] = train_df['image_id'].apply(create_absolute_path)\n",
    "    val_df['absolute_image_path'] = val_df['image_id'].apply(create_absolute_path)\n",
    "    test_df['absolute_image_path'] = test_df['image_id'].apply(create_absolute_path)\n",
    "    \n",
    "    non_feature_cols.extend(['absolute_image_path'])\n",
    "\n",
    "    # --- 5. ğŸ—‘ï¸ æ•°æ®æ¸…æ´— (Step 5) ---\n",
    "    print(\" æ­£åœ¨è¿‡æ»¤æ— æ•ˆè¡Œ (åŸºäºè·¯å¾„å’Œæ ‡ç­¾)...\")\n",
    "    initial_train, initial_val, initial_test = len(train_df), len(val_df), len(test_df)\n",
    "    \n",
    "    train_df = train_df.dropna(subset=['absolute_image_path', label_column])\n",
    "    val_df = val_df.dropna(subset=['absolute_image_path', label_column])\n",
    "    test_df = test_df.dropna(subset=['absolute_image_path', label_column])\n",
    "    \n",
    "    print(f\" > è®­ç»ƒé›†ç§»é™¤ {initial_train - len(train_df)} è¡Œ\")\n",
    "    print(f\" > éªŒè¯é›†ç§»é™¤ {initial_val - len(val_df)} è¡Œ\")\n",
    "    print(f\" > æµ‹è¯•é›†ç§»é™¤ {initial_test - len(test_df)} è¡Œ\")\n",
    "\n",
    "    # --- 6. âœï¸ å¡«å……ç¼ºå¤±å€¼ (Step 6) ---\n",
    "    print(\" æ­£åœ¨å¤„ç†ç¼ºå¤±å€¼/ç‰¹æ®Šç¼–ç ...\")\n",
    "\n",
    "    # (è¿ç»­ç‰¹å¾éƒ¨åˆ†è·³è¿‡)\n",
    "    for col in final_continuous_cols:\n",
    "        mean_val = train_df[col].mean()\n",
    "        train_df[col] = train_df[col].fillna(mean_val)\n",
    "        val_df[col] = val_df[col].fillna(mean_val)\n",
    "        test_df[col] = test_df[col].fillna(mean_val)\n",
    "        \n",
    "    # **CelebA ç‰¹å®šæ­¥éª¤**ï¼š\n",
    "    # *** 6. å‡è®¾ï¼šæˆ‘ä»¬å‡è®¾ .csv ä»ä½¿ç”¨ -1/1 ç¼–ç  ***\n",
    "    # (å¦‚æœæ‚¨çš„ .csv å·²æ˜¯ 0/1ï¼Œå¯ä»¥æ³¨é‡Šæ‰ .replace(-1, 0) è¿™ä¸€è¡Œ)\n",
    "    print(\" > [CelebA ç‰¹å®š] è½¬æ¢å±æ€§ï¼šå°† -1 æ˜ å°„åˆ° 0...\")\n",
    "    all_attr_cols = final_categorical_cols + [label_column]\n",
    "    for col in all_attr_cols:\n",
    "        train_df[col] = train_df[col].replace(-1, 0)\n",
    "        val_df[col] = val_df[col].replace(-1, 0)\n",
    "        test_df[col] = test_df[col].replace(-1, 0)\n",
    "        # æ— ç¼ºå¤±å€¼ï¼Œä¸éœ€è¦å¤„ç†\n",
    "        \n",
    "    # --- 7. ğŸ”¢ ç‰¹å¾å·¥ç¨‹ (ç±»åˆ«) (Step 7) ---\n",
    "    print(\" æ­£åœ¨å¤„ç†ç±»åˆ«ç‰¹å¾...\")\n",
    "    \n",
    "    # *** 7. å·²ä¿®å¤ï¼šåˆå§‹åŒ–ä¸ºç©ºåˆ—è¡¨ ***\n",
    "    field_lengths = []\n",
    "    \n",
    "    print(\" > CelebA å±æ€§å·²ä¸º 0/1 ç¼–ç ï¼Œè·³è¿‡.cat.codesã€‚\")\n",
    "    print(\" > æ­£åœ¨è®¡ç®—åŸºæ•° (Cardinalities)...\")\n",
    "\n",
    "    for col in final_categorical_cols:\n",
    "        all_values = pd.concat([train_df[col], val_df[col], test_df[col]])\n",
    "        cardinality = all_values.nunique()\n",
    "        # å¢åŠ ä¸€ä¸ªæ£€æŸ¥ï¼Œä»¥é˜² 'MISSING' çœŸçš„è¢«å¡«å……äº†\n",
    "        if \"MISSING\" in all_values.unique():\n",
    "            print(f\" > è­¦å‘Šï¼šç‰¹å¾ {col} åŒ…å« 'MISSING' å€¼ã€‚\")\n",
    "        field_lengths.append(cardinality)\n",
    "        \n",
    "    print(f\" > {len(final_categorical_cols)} ä¸ªç±»åˆ«ç‰¹å¾çš„åŸºæ•° (éƒ¨åˆ†): {field_lengths[:5]}...\")\n",
    "    \n",
    "    # --- 9. ğŸ”¢ ç‰¹å¾å·¥ç¨‹ (è¿ç»­) (Step 9) ---\n",
    "    print(\" æ­£åœ¨æ ‡å‡†åŒ–è¿ç»­ç‰¹å¾...\")\n",
    "\n",
    "    for _ in final_continuous_cols:\n",
    "        field_lengths.insert(0, 1) \n",
    "    \n",
    "    scaler = StandardScaler()\n",
    "    \n",
    "    if final_continuous_cols:\n",
    "        scaler.fit(train_df[final_continuous_cols])\n",
    "        train_df[final_continuous_cols] = scaler.transform(train_df[final_continuous_cols])\n",
    "        val_df[final_continuous_cols] = scaler.transform(val_df[final_continuous_cols])\n",
    "        test_df[final_continuous_cols] = scaler.transform(test_df[final_continuous_cols])\n",
    "        print(\" > è¿ç»­ç‰¹å¾å·²æ ‡å‡†åŒ–ã€‚\")\n",
    "    else:\n",
    "        print(\" > æ²¡æœ‰è¿ç»­ç‰¹å¾ï¼Œè·³è¿‡æ ‡å‡†åŒ–ã€‚\")\n",
    "\n",
    "    # --- 12. ğŸ–¼ï¸ å›¾åƒå¤„ç† (JEPG è½¬ NPY) (Step 12) ---\n",
    "    # *** 8. å·²ä¿®æ”¹ï¼šå¢åŠ  224x224 Resize å¹¶ä¿®å¤ npy_path Bug ***\n",
    "    print(f\" æ­£åœ¨è½¬æ¢å›¾åƒ (JPG -> NPY, å¹¶ Resize åˆ° {IMAGE_SIZE}x{IMAGE_SIZE})...\")\n",
    "\n",
    "    def convert_jpg_to_npy(jpg_path):\n",
    "        \"\"\"\n",
    "        æ‰“å¼€ JPG, Resize, è½¬æ¢ä¸º NumPy æ•°ç»„, ä¿å­˜ä¸º.npy, å¹¶è¿”å›.npy è·¯å¾„\n",
    "        \"\"\"\n",
    "        if not os.path.exists(jpg_path):\n",
    "            warnings.warn(f\" > è­¦å‘Šï¼šæ‰¾ä¸åˆ° JPG æ–‡ä»¶ {jpg_path}ã€‚è·³è¿‡ã€‚\")\n",
    "            return None\n",
    "            \n",
    "        # *** 9. å·²ä¿®å¤ï¼šä½¿ç”¨ [0] æ¥è·å–æ­£ç¡®çš„æ–‡ä»¶åŸºå ***\n",
    "        npy_path = os.path.splitext(jpg_path)[0] + \".npy\"\n",
    "        \n",
    "        if os.path.exists(npy_path):\n",
    "            return npy_path  # è·³è¿‡å·²å­˜åœ¨çš„æ–‡ä»¶\n",
    "            \n",
    "        try:\n",
    "            with Image.open(jpg_path) as img:\n",
    "                # *** 10. æ–°å¢ï¼šResize æ­¥éª¤ ***\n",
    "                img_resized = img.resize((IMAGE_SIZE, IMAGE_SIZE), resample=LANCZOS_RESAMPLE)\n",
    "                \n",
    "                # è½¬æ¢ä¸º NumPy æ•°ç»„\n",
    "                if img_resized.mode == 'RGBA':\n",
    "                    img_resized = img_resized.convert('RGB')\n",
    "                    \n",
    "                img_array = np.array(img_resized)\n",
    "                np.save(npy_path, img_array)\n",
    "                return npy_path\n",
    "        except Exception as e:\n",
    "            warnings.warn(f\" > è­¦å‘Šï¼šå¤„ç† {jpg_path} æ—¶å‡ºé”™: {e}ã€‚è·³è¿‡ã€‚\")\n",
    "            return None\n",
    "\n",
    "    non_feature_cols.extend(['npy_path'])\n",
    "    \n",
    "    tqdm.pandas(desc=\" > è½¬æ¢ è®­ç»ƒé›† å›¾åƒ\")\n",
    "    train_df['npy_path'] = train_df['absolute_image_path'].progress_apply(convert_jpg_to_npy)\n",
    "    \n",
    "    tqdm.pandas(desc=\" > è½¬æ¢ éªŒè¯é›† å›¾åƒ\")\n",
    "    val_df['npy_path'] = val_df['absolute_image_path'].progress_apply(convert_jpg_to_npy)\n",
    "    \n",
    "    tqdm.pandas(desc=\" > è½¬æ¢ æµ‹è¯•é›† å›¾åƒ\")\n",
    "    test_df['npy_path'] = test_df['absolute_image_path'].progress_apply(convert_jpg_to_npy)\n",
    "\n",
    "    # å†æ¬¡æ¸…æ´—ï¼šä¸¢å¼ƒè½¬æ¢å¤±è´¥çš„å›¾åƒ\n",
    "    train_df = train_df.dropna(subset=['npy_path'])\n",
    "    val_df = val_df.dropna(subset=['npy_path'])\n",
    "    test_df = test_df.dropna(subset=['npy_path'])\n",
    "    \n",
    "    print(\" > å›¾åƒè½¬æ¢å®Œæˆã€‚\")\n",
    "\n",
    "    # --- 10, 11, 12. ğŸ’¾ ä¿å­˜è¾“å‡º ---\n",
    "    print(\"[Final] æ­£åœ¨ä¿å­˜æ‰€æœ‰å¤„ç†åçš„æ–‡ä»¶...\")\n",
    "\n",
    "    final_feature_columns = final_continuous_cols + final_categorical_cols\n",
    "    \n",
    "    data_splits = {\n",
    "        'train': train_df,\n",
    "        'val': val_df,\n",
    "        'test': test_df\n",
    "    }\n",
    "    \n",
    "    for split_name, df in data_splits.items():\n",
    "        \n",
    "        # (Step 10) ä¿å­˜è¡¨æ ¼ç‰¹å¾\n",
    "        features_path = os.path.join(output_dir, f\"{split_name}_features.csv\")\n",
    "        df[final_feature_columns].to_csv(features_path, index=False, header=False)\n",
    "        \n",
    "        # (Step 11) ä¿å­˜æ ‡ç­¾\n",
    "        labels_path = os.path.join(output_dir, f\"{split_name}_labels.pt\")\n",
    "        labels_tensor = torch.tensor(df[label_column].values, dtype=torch.int64)\n",
    "        torch.save(labels_tensor, labels_path)\n",
    "        \n",
    "        # (Step 12) ä¿å­˜å›¾åƒè·¯å¾„\n",
    "        paths_path = os.path.join(output_dir, f\"{split_name}_paths.pt\")\n",
    "        npy_path_list = df['npy_path'].tolist()\n",
    "        torch.save(npy_path_list, paths_path)\n",
    "\n",
    "    # (Step 10) ä¿å­˜å­—æ®µé•¿åº¦\n",
    "    lengths_path = os.path.join(output_dir, \"tabular_lengths.pt\")\n",
    "    torch.save(field_lengths, lengths_path)\n",
    "\n",
    "    print(\"-\" * 30)\n",
    "    print(\"ğŸ‰ é¢„å¤„ç†å…¨éƒ¨å®Œæˆï¼ ğŸ‰\")\n",
    "    print(f\"è¾“å‡ºæ–‡ä»¶å·²ä¿å­˜åˆ°: {output_dir}\")\n",
    "    print(f\" > æ ‡ç­¾ (Label): {label_column}\")\n",
    "    print(f\" > è¡¨æ ¼ç‰¹å¾: {len(final_feature_columns)} ä¸ªå…¶ä»–å±æ€§\")\n",
    "    print(f\" > åˆ’åˆ†æ¯”ä¾‹: 8:1:1 (åˆ†å±‚)\")\n",
    "    print(f\" > å›¾åƒå¤„ç†: å·²ç¼©æ”¾è‡³ {IMAGE_SIZE}x{IMAGE_SIZE} å¹¶è½¬ä¸º .npy\")\n",
    "    print(\"-\" * 30)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # è¿è¡Œä¸»é¢„å¤„ç†å‡½æ•°\n",
    "    preprocess_celeba_flow(\n",
    "        attr_file=ATTR_FILE,\n",
    "        base_image_dir=BASE_IMAGE_DIR,\n",
    "        output_dir=OUTPUT_DIR,\n",
    "        label_column=LABEL_COLUMN\n",
    "    )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "charms",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
