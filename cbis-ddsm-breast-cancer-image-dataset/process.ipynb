{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0291c497",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import pandas as pd\n",
    "\n",
    "# ================== 1ï¸âƒ£ è·¯å¾„é…ç½® (è¯·æ ¹æ®æ‚¨çš„å®é™…è·¯å¾„ä¿®æ”¹) ==================\n",
    "# å‡è®¾æ‰€æœ‰CSVæ–‡ä»¶éƒ½åœ¨è¿™ä¸ªBASE_DIRä¸‹\n",
    "BASE_DIR = \"/mnt/hdd/jiazy/cbis-ddsm-breast-cancer-image-dataset/csv\"\n",
    "\n",
    "# è¾“å…¥æ–‡ä»¶\n",
    "dicom_path = os.path.join(BASE_DIR, \"dicom_info.csv\")\n",
    "calc_train_path = os.path.join(BASE_DIR, \"calc_case_description_train_set.csv\")\n",
    "mass_train_path = os.path.join(BASE_DIR, \"mass_case_description_train_set.csv\")\n",
    "calc_test_path = os.path.join(BASE_DIR, \"calc_case_description_test_set.csv\")\n",
    "mass_test_path = os.path.join(BASE_DIR, \"mass_case_description_test_set.csv\")\n",
    "\n",
    "# è¾“å‡ºæ–‡ä»¶\n",
    "OUTPUT_TRAIN_PATH = \"./train_cleaned.csv\"\n",
    "OUTPUT_TEST_PATH = \"./test_cleaned.csv\"\n",
    "\n",
    "# ================== 2ï¸âƒ£ è¾…åŠ©å‡½æ•° (æ¥è‡ªä»£ç ä¸€) ==================\n",
    "\n",
    "def extract_uids(path):\n",
    "    \"\"\"ä»è·¯å¾„ä¸­æå– StudyInstanceUID / SeriesInstanceUID\"\"\"\n",
    "    uids = re.findall(r\"1\\.3\\.6\\.1\\.4\\.1\\.9590\\.[\\d\\.]+\", str(path))\n",
    "    if len(uids) >= 2:\n",
    "        return pd.Series({\"StudyInstanceUID\": uids[0], \"SeriesInstanceUID\": uids[1]})\n",
    "    else:\n",
    "        return pd.Series({\"StudyInstanceUID\": None, \"SeriesInstanceUID\": None})\n",
    "\n",
    "def normalize_pathology(x):\n",
    "    \"\"\"æ ‡å‡†åŒ–ç—…ç†æ ‡ç­¾\"\"\"\n",
    "    x = str(x).strip().upper()\n",
    "    if \"MALIGNANT\" in x:\n",
    "        return \"MALIGNANT\"\n",
    "    elif \"BENIGN_WITHOUT_CALLBACK\" in x:\n",
    "        return \"BENIGN_WITHOUT_CALLBACK\"\n",
    "    elif \"BENIGN\" in x:\n",
    "        return \"BENIGN\"\n",
    "    return x\n",
    "\n",
    "def merge_with_dicom(df, dicom, lesion_type):\n",
    "    \"\"\"æ ¹æ® UID åŒ¹é… dicom_infoï¼Œè¿”å›åŒ…å«å›¾åƒè·¯å¾„çš„å®Œæ•´ DataFrame (æ¥è‡ªä»£ç ä¸€)\"\"\"\n",
    "    df[\"lesion_type\"] = lesion_type\n",
    "\n",
    "    # æå– UID\n",
    "    uid_info = df[\"image file path\"].apply(extract_uids)\n",
    "    df = pd.concat([df, uid_info], axis=1)\n",
    "\n",
    "    # æ ‡å‡†åŒ–æ ‡ç­¾\n",
    "    df[\"pathology\"] = df[\"pathology\"].apply(normalize_pathology)\n",
    "\n",
    "    # è¿‡æ»¤ full mammogram (æ¥è‡ªä»£ç ä¸€)\n",
    "    dicom_full = dicom[dicom[\"SeriesDescription\"].str.contains(\"full\", case=False, na=False)]\n",
    "    dicom_unique = dicom_full.drop_duplicates(subset=[\"StudyInstanceUID\", \"SeriesInstanceUID\"], keep=\"first\")\n",
    "\n",
    "    merged = df.merge(\n",
    "        dicom_unique, # åˆå¹¶æ‰€æœ‰ dicom_info åˆ—ï¼Œä»¥ä¾¿åç»­æ¸…æ´—\n",
    "        on=[\"StudyInstanceUID\", \"SeriesInstanceUID\"],\n",
    "        how=\"left\"\n",
    "    )\n",
    "\n",
    "    before = len(merged)\n",
    "    # ç¡®å®š 'image_path' åˆ—çš„æ­£ç¡®åç§° (åˆå¹¶åå¯èƒ½å¸¦åç¼€ _y)\n",
    "    image_path_col = 'image_path_y' if 'image_path_y' in merged.columns else 'image_path'\n",
    "    \n",
    "    # â­ã€å·²ä¿®æ”¹ã€‘é‡å‘½åä¸ºæ ‡å‡†çš„æœ€ç»ˆè·¯å¾„åˆ—\n",
    "    merged = merged.rename(columns={image_path_col: 'image_path_final'})\n",
    "    \n",
    "    # â­ã€å·²ä¿®æ”¹ã€‘ä½¿ç”¨æ–°åˆ—å dropna\n",
    "    merged = merged.dropna(subset=['image_path_final']).reset_index(drop=True)\n",
    "    \n",
    "    after = len(merged)\n",
    "    print(f\"âœ… {lesion_type}: æˆåŠŸåŒ¹é… {after}/{before} æ¡è®°å½• ({after/before:.1%})\")\n",
    "\n",
    "    return merged\n",
    "\n",
    "# ================== 3ï¸âƒ£ é¢„æ¸…æ´—å‡½æ•° (æ¥è‡ªä»£ç äºŒ) ==================\n",
    "\n",
    "def clean_calc_df(df):\n",
    "    \"\"\"åº”ç”¨ä»£ç äºŒçš„ç­–ç•¥æ¸…æ´— calc_case_description\"\"\"\n",
    "    print(f\"æ¸…æ´— calc_case_df (åŸå§‹æ¡æ•°: {len(df)})\")\n",
    "    # é‡å‘½å (ä»£ç äºŒ, æ­¥éª¤ 5)\n",
    "    df = df.rename(columns={\n",
    "        'calc type':'calc_type',\n",
    "        'calc distribution':'calc_distribution',\n",
    "        'image view':'image_view',\n",
    "        'left or right breast':'left_or_right_breast',\n",
    "        'breast density':'breast_density',\n",
    "        'abnormality type':'abnormality_type'\n",
    "    })\n",
    "    \n",
    "    # å¡«å……ç¼ºå¤±å€¼ (ä»£ç äºŒ, æ­¥éª¤ 5)\n",
    "    if 'calc_type' in df.columns:\n",
    "        df['calc_type'] = df['calc_type'].bfill()\n",
    "    if 'calc_distribution' in df.columns:\n",
    "        df['calc_distribution'] = df['calc_distribution'].bfill()\n",
    "        \n",
    "    return df\n",
    "\n",
    "def clean_mass_df(df):\n",
    "    \"\"\"åº”ç”¨ä»£ç äºŒçš„ç­–ç•¥æ¸…æ´— mass_case_description\"\"\"\n",
    "    print(f\"æ¸…æ´— mass_case_df (åŸå§‹æ¡æ•°: {len(df)})\")\n",
    "    # é‡å‘½å (ä»£ç äºŒ, æ­¥éª¤ 6)\n",
    "    df = df.rename(columns={\n",
    "        'mass shape':'mass_shape',\n",
    "        'mass margins':'mass_margins',\n",
    "        'image view':'image_view',\n",
    "        'left or right breast':'left_or_right_breast',\n",
    "        'abnormality type':'abnormality_type'\n",
    "    })\n",
    "    \n",
    "    # å¡«å……ç¼ºå¤±å€¼ (ä»£ç äºŒ, æ­¥éª¤ 6)\n",
    "    if 'mass_shape' in df.columns:\n",
    "        df['mass_shape'] = df['mass_shape'].bfill()\n",
    "    if 'mass_margins' in df.columns:\n",
    "        df['mass_margins'] = df['mass_margins'].bfill()\n",
    "        \n",
    "    return df\n",
    "\n",
    "# ================== 4ï¸âƒ£ åæ¸…æ´—å‡½æ•° (æ¥è‡ªä»£ç äºŒ) ==================\n",
    "\n",
    "def post_merge_clean(df):\n",
    "    \"\"\"åº”ç”¨ä»£ç äºŒçš„ç­–ç•¥æ¸…æ´—åˆå¹¶åçš„ DataFrame\"\"\"\n",
    "    \n",
    "    # å¡«å……ç¼ºå¤±å€¼ (ä»£ç äºŒ, æ­¥éª¤ 4)\n",
    "    if 'SeriesDescription' in df.columns:\n",
    "        df['SeriesDescription'] = df['SeriesDescription'].bfill()\n",
    "    if 'Laterality' in df.columns:\n",
    "        df['Laterality'] = df['Laterality'].bfill()\n",
    "\n",
    "    # â­ã€å·²ä¿®æ”¹ã€‘å°†æ‰€æœ‰æ‚¨ä¸å¸Œæœ›çš„åˆ—åŠ å…¥åˆ é™¤åˆ—è¡¨\n",
    "    COLS_TO_DROP = [\n",
    "        # Columns from code 2 (UIDs, dates, etc.)\n",
    "        'PatientBirthDate','AccessionNumber','Columns','ContentDate','ContentTime',\n",
    "        'PatientSex','ReferringPhysicianName','Rows','SOPClassUID',\n",
    "        'SOPInstanceUID','StudyDate','StudyID','StudyInstanceUID','StudyTime',\n",
    "        'InstanceNumber','SeriesInstanceUID','SeriesNumber',\n",
    "        \n",
    "        # Original paths and redundant columns\n",
    "        'image file path', \n",
    "        'cropped image file path', 'ROI mask file path',\n",
    "        'image_path_x', 'image_path', \n",
    "        \n",
    "        # Expert features (from previous request)\n",
    "        'calc_type', 'calc_distribution', 'mass_shape', 'mass_margins',\n",
    "        \n",
    "        # â­ æ–°å¢: (from this request)\n",
    "        'patient_id',       # from calc/mass files\n",
    "        'lesion_type',      # created by this script\n",
    "        'PatientID',        # from dicom_info\n",
    "        'PatientName',       # from dicom_info\n",
    "        'abnormality_type',\n",
    "        'pathology',\n",
    "        'file_path',\n",
    "        'Laterality',\n",
    "        'PatientOrientation',\n",
    "    ]\n",
    "    \n",
    "    # åªåˆ é™¤æ•°æ®æ¡†ä¸­å®é™…å­˜åœ¨çš„åˆ—\n",
    "    cols_exist = [col for col in COLS_TO_DROP if col in df.columns]\n",
    "    df = df.drop(columns=cols_exist)\n",
    "    \n",
    "    return df\n",
    "\n",
    "# ================== 5ï¸âƒ£ ä¸»æ‰§è¡Œæµç¨‹ ==================\n",
    "\n",
    "print(\"--- 1. åŠ è½½æ•°æ® ---\")\n",
    "dicom = pd.read_csv(dicom_path)\n",
    "calc_train = pd.read_csv(calc_train_path)\n",
    "mass_train = pd.read_csv(mass_train_path)\n",
    "calc_test = pd.read_csv(calc_test_path)\n",
    "mass_test = pd.read_csv(mass_test_path)\n",
    "\n",
    "print(f\"Dicom info æ¡æ•°: {len(dicom)}\")\n",
    "print(f\"Calc train æ¡æ•°: {len(calc_train)}, Calc test æ¡æ•°: {len(calc_test)}\")\n",
    "print(f\"Mass train æ¡æ•°: {len(mass_train)}, Mass test æ¡æ•°: {len(mass_test)}\")\n",
    "\n",
    "print(\"\\n--- 2. é¢„æ¸…æ´— (ä»£ç äºŒé€»è¾‘) ---\")\n",
    "calc_train_cleaned = clean_calc_df(calc_train)\n",
    "mass_train_cleaned = clean_mass_df(mass_train)\n",
    "calc_test_cleaned = clean_calc_df(calc_test)\n",
    "mass_test_cleaned = clean_mass_df(mass_test)\n",
    "\n",
    "print(\"\\n--- 3. åˆå¹¶Dicom (ä»£ç ä¸€é€»è¾‘) ---\")\n",
    "merged_calc_train = merge_with_dicom(calc_train_cleaned, dicom, \"calcification\")\n",
    "merged_mass_train = merge_with_dicom(mass_train_cleaned, dicom, \"mass\")\n",
    "merged_calc_test = merge_with_dicom(calc_test_cleaned, dicom, \"calcification\")\n",
    "merged_mass_test = merge_with_dicom(mass_test_cleaned, dicom, \"mass\")\n",
    "\n",
    "print(\"\\n--- 4. åˆ›å»º4åˆ†ç±»æ ‡ç­¾ (ä»£ç ä¸€é€»è¾‘) ---\")\n",
    "label_map = {\n",
    "    (\"calcification\", \"MALIGNANT\"): 0,\n",
    "    (\"calcification\", \"BENIGN\"): 1,\n",
    "    (\"calcification\", \"BENIGN_WITHOUT_CALLBACK\"): 1,\n",
    "    (\"mass\", \"MALIGNANT\"): 2,\n",
    "    (\"mass\", \"BENIGN\"): 3,\n",
    "    (\"mass\", \"BENIGN_WITHOUT_CALLBACK\"): 3,\n",
    "}\n",
    "\n",
    "for df in [merged_calc_train, merged_mass_train, merged_calc_test, merged_mass_test]:\n",
    "    df[\"label_4c\"] = df.apply(\n",
    "        lambda r: label_map.get((r[\"lesion_type\"], r[\"pathology\"]), None), axis=1\n",
    "    )\n",
    "\n",
    "print(\"--- 5. åˆå¹¶ Train / Test é›† ---\")\n",
    "train_df = pd.concat([merged_calc_train, merged_mass_train], ignore_index=True)\n",
    "test_df = pd.concat([merged_calc_test, merged_mass_test], ignore_index=True)\n",
    "\n",
    "# æ‰“ä¹±è®­ç»ƒé›† (æ¥è‡ªä»£ç ä¸€)\n",
    "train_df = train_df.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "# æµ‹è¯•é›†é€šå¸¸ä¸éœ€è¦æ‰“ä¹±\n",
    "test_df = test_df.reset_index(drop=True)\n",
    "\n",
    "\n",
    "print(f\"Train set æ€»æ ·æœ¬æ•°: {len(train_df)}\")\n",
    "print(f\"Test set æ€»æ ·æœ¬æ•°: {len(test_df)}\")\n",
    "\n",
    "print(\"\\n--- 6. åæ¸…æ´— (ä»£ç äºŒé€»è¾‘) ---\")\n",
    "train_df_final = post_merge_clean(train_df)\n",
    "test_df_final = post_merge_clean(test_df)\n",
    "\n",
    "print(f\"Train set æ¸…æ´—åå‰©ä½™åˆ—æ•°: {len(train_df_final.columns)}\")\n",
    "print(f\"Test set æ¸…æ´—åå‰©ä½™åˆ—æ•°: {len(test_df_final.columns)}\")\n",
    "\n",
    "# æ›¿æ¢è·¯å¾„\n",
    "# 1. ç¡®ä¿ image_path_final åˆ—æ˜¯å­—ç¬¦ä¸²ç±»å‹ï¼Œä»¥é¿å… .str è®¿é—®å™¨å‡ºé”™\n",
    "train_df_final['image_path_final'] = train_df_final['image_path_final'].astype(str)\n",
    "test_df_final['image_path_final'] = test_df_final['image_path_final'].astype(str)\n",
    "\n",
    "# 2. æ‰§è¡Œå­—ç¬¦ä¸²æ›¿æ¢\n",
    "train_df_final['image_path_final'] = train_df_final['image_path_final'].str.replace(\n",
    "    \"CBIS-DDSM\", \n",
    "    \"cbis-ddsm-breast-cancer-image-dataset\"\n",
    ")\n",
    "test_df_final['image_path_final'] = test_df_final['image_path_final'].str.replace(\n",
    "    \"CBIS-DDSM\", \n",
    "    \"cbis-ddsm-breast-cancer-image-dataset\"\n",
    ")\n",
    "\n",
    "print(\"\\n--- 7. ä¿å­˜ç»“æœ ---\")\n",
    "train_df_final.to_csv(OUTPUT_TRAIN_PATH, index=False)\n",
    "test_df_final.to_csv(OUTPUT_TEST_PATH, index=False)\n",
    "\n",
    "print(f\"âœ… å·²ä¿å­˜è®­ç»ƒé›†: {OUTPUT_TRAIN_PATH}\")\n",
    "print(f\"âœ… å·²ä¿å­˜æµ‹è¯•é›†: {OUTPUT_TEST_PATH}\")\n",
    "\n",
    "print(\"\\n--- æœ€ç»ˆè®­ç»ƒé›† æ ‡ç­¾åˆ†å¸ƒ (label_4c) ---\")\n",
    "print(train_df_final[\"label_4c\"].value_counts(dropna=False))\n",
    "\n",
    "print(\"\\n--- æœ€ç»ˆè®­ç»ƒé›† ç¤ºä¾‹æ•°æ® (å·²åˆ é™¤æ‰€æœ‰æŒ‡å®šåˆ—) ---\")\n",
    "print(train_df_final.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "29785b1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_188210/3885103284.py:222: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise in a future error of pandas. Value '[ 0.30490534  0.30490534 -1.17317184 ...  0.30490534 -2.16027213\n",
      "  0.30490534]' has dtype incompatible with int64, please explicitly cast to a compatible dtype first.\n",
      "  train_split.loc[:, final_continuous_cols] = scaler.transform(train_split[final_continuous_cols])\n",
      "/tmp/ipykernel_188210/3885103284.py:223: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise in a future error of pandas. Value '[ 0.30490534  0.30490534  0.30490534  0.30490534  0.30490534  0.30490534\n",
      "  0.30490534  0.30490534  0.30490534  0.30490534  0.30490534  0.30490534\n",
      " -2.54214304  0.30490534 -3.51997962  0.30490534  0.30490534  0.30490534\n",
      "  0.30490534  0.30490534  0.30490534  0.30490534  0.30490534 -2.37539618\n",
      "  0.30490534  0.30490534 -3.46336803  0.30490534  0.30490534  0.30490534\n",
      "  0.30490534  0.30490534  0.30490534  0.30490534  0.30490534  0.30490534\n",
      "  0.30490534  0.30490534  0.30490534  0.30490534 -2.28481763 -2.74954733\n",
      "  0.30490534  0.30490534  0.30439069  0.30490534  0.30490534  0.30490534\n",
      "  0.30490534  0.30490534  0.30490534  0.30490534  0.30490534  0.30490534\n",
      "  0.30490534  0.30490534  0.30490534  0.30490534  0.30490534  0.30490534\n",
      "  0.30490534  0.30490534  0.30490534  0.30490534  0.30490534  0.30490534\n",
      "  0.30490534  0.30490534  0.30490534  0.30490534  0.30490534  0.30490534\n",
      "  0.30490534  0.30490534  0.30490534  0.30490534  0.30490534  0.30490534\n",
      "  0.30490534  0.30490534  0.30490534  0.30490534  0.30490534  0.30490534\n",
      "  0.30490534  0.30490534  0.30490534  0.30490534  0.30490534  0.30490534\n",
      "  0.30490534 -1.8319249   0.30490534  0.30490534  0.30490534  0.30490534\n",
      "  0.30490534  0.30490534  0.30490534  0.30490534  0.30490534  0.30439069\n",
      " -2.62500183  0.30490534  0.30490534  0.30490534  0.30490534  0.30439069\n",
      "  0.30490534  0.30490534  0.30490534  0.30490534  0.30439069  0.30490534\n",
      "  0.30490534  0.30490534  0.30490534  0.30490534  0.30490534  0.30490534\n",
      "  0.30490534  0.30490534  0.30490534  0.30490534  0.30490534  0.30490534\n",
      " -2.55706792  0.30490534  0.30490534  0.30490534 -2.97599369  0.30490534\n",
      "  0.30490534  0.30490534  0.30439069  0.30490534  0.30490534  0.30490534\n",
      "  0.30490534  0.30490534  0.30490534  0.30490534  0.30490534  0.30490534\n",
      "  0.30490534 -4.29041191  0.30490534  0.30439069  0.30490534  0.30490534\n",
      "  0.30490534  0.30490534 -2.44024218  0.30490534  0.30490534  0.30490534\n",
      "  0.30490534  0.30490534  0.30490534  0.30490534  0.30490534  0.30490534\n",
      "  0.30490534 -2.88541515  0.30490534  0.30490534  0.30490534  0.30490534\n",
      "  0.30490534  0.30490534  0.30490534  0.30490534  0.30490534  0.30490534\n",
      "  0.30490534  0.30490534 -1.9564704   0.30490534  0.30490534  0.30490534\n",
      "  0.30490534  0.30490534  0.30490534  0.30490534  0.30490534  0.30490534\n",
      "  0.30490534  0.30490534  0.30490534  0.30490534  0.30439069  0.30490534\n",
      "  0.30490534  0.30490534 -2.88541515  0.30490534  0.30490534  0.30490534\n",
      "  0.30490534  0.30490534  0.30490534  0.30490534  0.30490534  0.30490534\n",
      "  0.30490534 -2.39495291  0.30490534  0.30490534  0.30490534  0.30490534\n",
      " -1.2426497   0.30490534  0.30490534  0.30490534  0.30490534  0.30490534\n",
      "  0.30490534  0.30490534  0.30490534  0.30490534  0.30490534  0.30490534\n",
      "  0.30490534  0.30490534  0.30490534  0.30490534  0.30490534  0.30490534\n",
      "  0.30490534  0.30490534  0.30490534 -3.09693664  0.30490534 -2.16850654\n",
      "  0.30490534  0.30490534  0.30490534  0.30490534  0.30490534  0.30490534\n",
      "  0.30490534  0.30490534  0.30490534  0.30490534  0.30490534  0.30490534\n",
      "  0.30490534  0.30490534  0.30490534 -3.28221094  0.30490534  0.30490534\n",
      "  0.30490534  0.30490534  0.30490534  0.30490534  0.30490534  0.30490534\n",
      "  0.30490534  0.30490534  0.30490534 -7.51984587  0.30490534 -1.89985881\n",
      "  0.30490534  0.30490534  0.30490534  0.30490534  0.30490534  0.30490534\n",
      "  0.30490534  0.30490534  0.30490534  0.30490534  0.30490534  0.30490534\n",
      "  0.30490534  0.30490534  0.30490534  0.30490534  0.30490534  0.30490534\n",
      "  0.30490534  0.30490534  0.30490534  0.30490534 -2.35275154  0.30490534\n",
      "  0.30490534  0.30490534  0.30490534  0.30490534  0.30490534  0.30490534\n",
      "  0.30490534  0.30490534 -2.7948366   0.30490534  0.30490534  0.30439069\n",
      "  0.30490534  0.30490534  0.30490534  0.30490534  0.30490534  0.30490534\n",
      "  0.30490534  0.30490534  0.30490534  0.30490534 -2.39804081  0.30490534\n",
      "  0.30490534  0.30490534 -1.87412627  0.30490534  0.30490534  0.30490534\n",
      "  0.30490534 -2.31621133  0.30490534  0.30490534  0.30439069  0.30490534\n",
      "  0.30490534 -1.91118113  0.30439069  0.30490534  0.30490534  0.30490534\n",
      "  0.30490534  0.30490534  0.30490534  0.30490534  0.30490534  0.30490534\n",
      "  0.30490534  0.30490534  0.30490534  0.30490534 -2.85144819  0.30490534\n",
      "  0.30490534  0.30490534  0.30439069  0.30490534  0.30490534  0.30490534\n",
      "  0.30490534  0.30490534  0.30490534  0.30490534  0.30490534  0.30490534\n",
      " -3.65584744  0.30490534  0.30490534  0.30490534  0.30439069  0.30490534\n",
      "  0.30490534  0.30439069  0.30490534  0.30490534  0.30490534  0.30439069\n",
      "  0.30490534  0.30490534  0.30490534  0.30490534  0.30490534  0.30490534\n",
      "  0.30490534  0.30490534 -6.07573564  0.30490534 -2.61367951  0.30490534\n",
      "  0.30490534 -3.76546807  0.30490534  0.30490534 -2.27349531  0.30490534\n",
      "  0.30490534  0.30490534  0.30490534  0.30490534  0.30490534  0.30490534\n",
      "  0.30490534  0.30490534 -1.66157547  0.30490534  0.30490534  0.30490534\n",
      " -2.9193821   0.30490534 -1.79795794  0.30439069  0.30490534  0.30490534\n",
      "  0.30490534  0.30490534  0.30490534  0.30490534  0.30490534  0.30490534\n",
      "  0.30439069 -2.95334906  0.30490534  0.30490534 -2.96621533  0.30490534\n",
      "  0.30490534  0.30490534  0.30490534  0.30490534  0.30490534  0.30490534\n",
      "  0.30490534  0.30490534  0.30490534  0.30490534  0.30490534  0.30490534\n",
      " -1.97911504  0.30490534 -1.74752216  0.30490534  0.30490534  0.30490534\n",
      " -1.19427252  0.30490534  0.30490534  0.30490534 -1.62760852  0.30439069\n",
      "  0.30490534  0.30490534  0.30490534  0.30490534  0.30490534  0.30490534\n",
      "  0.30490534  0.30490534 -2.95334906  0.30490534  0.30490534  0.30490534\n",
      "  0.30490534  0.30490534 -2.80615892  0.30490534  0.30490534  0.30490534\n",
      "  0.30490534  0.30490534  0.30490534  0.30490534  0.30490534  0.30490534\n",
      "  0.30490534  0.30490534  0.30490534  0.30490534  0.30490534  0.30490534\n",
      " -2.51177865  0.30490534 -3.51997962  0.30490534  0.30490534  0.30490534\n",
      "  0.30490534  0.30490534  0.30490534  0.30490534 -3.13450615  0.30490534\n",
      "  0.30490534  0.30490534  0.30490534  0.30490534  0.30490534  0.30490534\n",
      "  0.30490534  0.30490534  0.30490534  0.30490534  0.30490534  0.30490534\n",
      "  0.30490534  0.30490534  0.30490534  0.30490534 -3.90133588  0.30490534\n",
      "  0.30490534  0.30490534  0.30490534  0.30490534  0.30490534  0.30490534\n",
      "  0.30490534  0.30490534  0.30490534  0.30490534  0.30490534  0.30490534\n",
      "  0.30490534  0.30490534  0.30490534  0.30490534  0.30490534  0.30490534\n",
      "  0.30490534  0.30490534 -4.07528787 -4.7510244   0.30490534 -2.34142922\n",
      "  0.30490534  0.30490534  0.30490534  0.30490534  0.30490534  0.30490534\n",
      "  0.30490534 -2.10366054  0.30439069  0.30490534  0.30490534 -2.13762749\n",
      "  0.30490534  0.30490534  0.30490534  0.30490534  0.30490534  0.30490534\n",
      "  0.30490534 -3.3841118   0.30490534  0.30490534  0.30490534  0.30490534\n",
      "  0.30490534 -2.31878458  0.30490534  0.30490534 -3.51997962  0.30490534\n",
      "  0.30490534  0.30490534  0.30490534  0.30490534  0.30490534  0.30490534\n",
      "  0.30490534 -1.66157547  0.30490534  0.30490534  0.30490534  0.30490534\n",
      "  0.30490534 -0.89834829  0.30490534  0.30490534  0.30490534  0.30490534\n",
      "  0.30490534  0.30490534  0.30490534]' has dtype incompatible with int64, please explicitly cast to a compatible dtype first.\n",
      "  val_split.loc[:, final_continuous_cols]   = scaler.transform(val_split[final_continuous_cols])\n",
      "/tmp/ipykernel_188210/3885103284.py:268: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise in a future error of pandas. Value '[ 0.30490534  0.30490534 -1.17317184 ...  0.30490534 -2.16027213\n",
      "  0.30490534]' has dtype incompatible with int64, please explicitly cast to a compatible dtype first.\n",
      "  train_split_tip.loc[:, final_continuous_cols] = scaler.transform(train_split_tip[final_continuous_cols])\n",
      "/tmp/ipykernel_188210/3885103284.py:269: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise in a future error of pandas. Value '[ 0.30490534  0.30490534  0.30490534  0.30490534  0.30490534  0.30490534\n",
      "  0.30490534  0.30490534  0.30490534  0.30490534  0.30490534  0.30490534\n",
      " -2.54214304  0.30490534 -3.51997962  0.30490534  0.30490534  0.30490534\n",
      "  0.30490534  0.30490534  0.30490534  0.30490534  0.30490534 -2.37539618\n",
      "  0.30490534  0.30490534 -3.46336803  0.30490534  0.30490534  0.30490534\n",
      "  0.30490534  0.30490534  0.30490534  0.30490534  0.30490534  0.30490534\n",
      "  0.30490534  0.30490534  0.30490534  0.30490534 -2.28481763 -2.74954733\n",
      "  0.30490534  0.30490534  0.30439069  0.30490534  0.30490534  0.30490534\n",
      "  0.30490534  0.30490534  0.30490534  0.30490534  0.30490534  0.30490534\n",
      "  0.30490534  0.30490534  0.30490534  0.30490534  0.30490534  0.30490534\n",
      "  0.30490534  0.30490534  0.30490534  0.30490534  0.30490534  0.30490534\n",
      "  0.30490534  0.30490534  0.30490534  0.30490534  0.30490534  0.30490534\n",
      "  0.30490534  0.30490534  0.30490534  0.30490534  0.30490534  0.30490534\n",
      "  0.30490534  0.30490534  0.30490534  0.30490534  0.30490534  0.30490534\n",
      "  0.30490534  0.30490534  0.30490534  0.30490534  0.30490534  0.30490534\n",
      "  0.30490534 -1.8319249   0.30490534  0.30490534  0.30490534  0.30490534\n",
      "  0.30490534  0.30490534  0.30490534  0.30490534  0.30490534  0.30439069\n",
      " -2.62500183  0.30490534  0.30490534  0.30490534  0.30490534  0.30439069\n",
      "  0.30490534  0.30490534  0.30490534  0.30490534  0.30439069  0.30490534\n",
      "  0.30490534  0.30490534  0.30490534  0.30490534  0.30490534  0.30490534\n",
      "  0.30490534  0.30490534  0.30490534  0.30490534  0.30490534  0.30490534\n",
      " -2.55706792  0.30490534  0.30490534  0.30490534 -2.97599369  0.30490534\n",
      "  0.30490534  0.30490534  0.30439069  0.30490534  0.30490534  0.30490534\n",
      "  0.30490534  0.30490534  0.30490534  0.30490534  0.30490534  0.30490534\n",
      "  0.30490534 -4.29041191  0.30490534  0.30439069  0.30490534  0.30490534\n",
      "  0.30490534  0.30490534 -2.44024218  0.30490534  0.30490534  0.30490534\n",
      "  0.30490534  0.30490534  0.30490534  0.30490534  0.30490534  0.30490534\n",
      "  0.30490534 -2.88541515  0.30490534  0.30490534  0.30490534  0.30490534\n",
      "  0.30490534  0.30490534  0.30490534  0.30490534  0.30490534  0.30490534\n",
      "  0.30490534  0.30490534 -1.9564704   0.30490534  0.30490534  0.30490534\n",
      "  0.30490534  0.30490534  0.30490534  0.30490534  0.30490534  0.30490534\n",
      "  0.30490534  0.30490534  0.30490534  0.30490534  0.30439069  0.30490534\n",
      "  0.30490534  0.30490534 -2.88541515  0.30490534  0.30490534  0.30490534\n",
      "  0.30490534  0.30490534  0.30490534  0.30490534  0.30490534  0.30490534\n",
      "  0.30490534 -2.39495291  0.30490534  0.30490534  0.30490534  0.30490534\n",
      " -1.2426497   0.30490534  0.30490534  0.30490534  0.30490534  0.30490534\n",
      "  0.30490534  0.30490534  0.30490534  0.30490534  0.30490534  0.30490534\n",
      "  0.30490534  0.30490534  0.30490534  0.30490534  0.30490534  0.30490534\n",
      "  0.30490534  0.30490534  0.30490534 -3.09693664  0.30490534 -2.16850654\n",
      "  0.30490534  0.30490534  0.30490534  0.30490534  0.30490534  0.30490534\n",
      "  0.30490534  0.30490534  0.30490534  0.30490534  0.30490534  0.30490534\n",
      "  0.30490534  0.30490534  0.30490534 -3.28221094  0.30490534  0.30490534\n",
      "  0.30490534  0.30490534  0.30490534  0.30490534  0.30490534  0.30490534\n",
      "  0.30490534  0.30490534  0.30490534 -7.51984587  0.30490534 -1.89985881\n",
      "  0.30490534  0.30490534  0.30490534  0.30490534  0.30490534  0.30490534\n",
      "  0.30490534  0.30490534  0.30490534  0.30490534  0.30490534  0.30490534\n",
      "  0.30490534  0.30490534  0.30490534  0.30490534  0.30490534  0.30490534\n",
      "  0.30490534  0.30490534  0.30490534  0.30490534 -2.35275154  0.30490534\n",
      "  0.30490534  0.30490534  0.30490534  0.30490534  0.30490534  0.30490534\n",
      "  0.30490534  0.30490534 -2.7948366   0.30490534  0.30490534  0.30439069\n",
      "  0.30490534  0.30490534  0.30490534  0.30490534  0.30490534  0.30490534\n",
      "  0.30490534  0.30490534  0.30490534  0.30490534 -2.39804081  0.30490534\n",
      "  0.30490534  0.30490534 -1.87412627  0.30490534  0.30490534  0.30490534\n",
      "  0.30490534 -2.31621133  0.30490534  0.30490534  0.30439069  0.30490534\n",
      "  0.30490534 -1.91118113  0.30439069  0.30490534  0.30490534  0.30490534\n",
      "  0.30490534  0.30490534  0.30490534  0.30490534  0.30490534  0.30490534\n",
      "  0.30490534  0.30490534  0.30490534  0.30490534 -2.85144819  0.30490534\n",
      "  0.30490534  0.30490534  0.30439069  0.30490534  0.30490534  0.30490534\n",
      "  0.30490534  0.30490534  0.30490534  0.30490534  0.30490534  0.30490534\n",
      " -3.65584744  0.30490534  0.30490534  0.30490534  0.30439069  0.30490534\n",
      "  0.30490534  0.30439069  0.30490534  0.30490534  0.30490534  0.30439069\n",
      "  0.30490534  0.30490534  0.30490534  0.30490534  0.30490534  0.30490534\n",
      "  0.30490534  0.30490534 -6.07573564  0.30490534 -2.61367951  0.30490534\n",
      "  0.30490534 -3.76546807  0.30490534  0.30490534 -2.27349531  0.30490534\n",
      "  0.30490534  0.30490534  0.30490534  0.30490534  0.30490534  0.30490534\n",
      "  0.30490534  0.30490534 -1.66157547  0.30490534  0.30490534  0.30490534\n",
      " -2.9193821   0.30490534 -1.79795794  0.30439069  0.30490534  0.30490534\n",
      "  0.30490534  0.30490534  0.30490534  0.30490534  0.30490534  0.30490534\n",
      "  0.30439069 -2.95334906  0.30490534  0.30490534 -2.96621533  0.30490534\n",
      "  0.30490534  0.30490534  0.30490534  0.30490534  0.30490534  0.30490534\n",
      "  0.30490534  0.30490534  0.30490534  0.30490534  0.30490534  0.30490534\n",
      " -1.97911504  0.30490534 -1.74752216  0.30490534  0.30490534  0.30490534\n",
      " -1.19427252  0.30490534  0.30490534  0.30490534 -1.62760852  0.30439069\n",
      "  0.30490534  0.30490534  0.30490534  0.30490534  0.30490534  0.30490534\n",
      "  0.30490534  0.30490534 -2.95334906  0.30490534  0.30490534  0.30490534\n",
      "  0.30490534  0.30490534 -2.80615892  0.30490534  0.30490534  0.30490534\n",
      "  0.30490534  0.30490534  0.30490534  0.30490534  0.30490534  0.30490534\n",
      "  0.30490534  0.30490534  0.30490534  0.30490534  0.30490534  0.30490534\n",
      " -2.51177865  0.30490534 -3.51997962  0.30490534  0.30490534  0.30490534\n",
      "  0.30490534  0.30490534  0.30490534  0.30490534 -3.13450615  0.30490534\n",
      "  0.30490534  0.30490534  0.30490534  0.30490534  0.30490534  0.30490534\n",
      "  0.30490534  0.30490534  0.30490534  0.30490534  0.30490534  0.30490534\n",
      "  0.30490534  0.30490534  0.30490534  0.30490534 -3.90133588  0.30490534\n",
      "  0.30490534  0.30490534  0.30490534  0.30490534  0.30490534  0.30490534\n",
      "  0.30490534  0.30490534  0.30490534  0.30490534  0.30490534  0.30490534\n",
      "  0.30490534  0.30490534  0.30490534  0.30490534  0.30490534  0.30490534\n",
      "  0.30490534  0.30490534 -4.07528787 -4.7510244   0.30490534 -2.34142922\n",
      "  0.30490534  0.30490534  0.30490534  0.30490534  0.30490534  0.30490534\n",
      "  0.30490534 -2.10366054  0.30439069  0.30490534  0.30490534 -2.13762749\n",
      "  0.30490534  0.30490534  0.30490534  0.30490534  0.30490534  0.30490534\n",
      "  0.30490534 -3.3841118   0.30490534  0.30490534  0.30490534  0.30490534\n",
      "  0.30490534 -2.31878458  0.30490534  0.30490534 -3.51997962  0.30490534\n",
      "  0.30490534  0.30490534  0.30490534  0.30490534  0.30490534  0.30490534\n",
      "  0.30490534 -1.66157547  0.30490534  0.30490534  0.30490534  0.30490534\n",
      "  0.30490534 -0.89834829  0.30490534  0.30490534  0.30490534  0.30490534\n",
      "  0.30490534  0.30490534  0.30490534]' has dtype incompatible with int64, please explicitly cast to a compatible dtype first.\n",
      "  val_split_tip.loc[:, final_continuous_cols]   = scaler.transform(val_split_tip[final_continuous_cols])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "æ•°æ®æº (CSV): /mnt/hdd/jiazy/cbis-ddsm-breast-cancer-image-dataset/\n",
      "è¾“å‡ºç›®å½• (Processed): /mnt/hdd/jiazy/cbis-ddsm-breast-cancer-image-dataset/features\n",
      "\n",
      "Step 1/11: åŠ è½½å·²æ¸…ç†çš„ train_cleaned.csv å’Œ test_cleaned.csv...\n",
      "    Train DF (loaded): 2864 è¡Œ, Test DF (loaded): 422 è¡Œ\n",
      "Step 2/11: å®šä¹‰ç‰¹å¾ç±»å‹ (åŸºäºæ‚¨çš„è¦æ±‚)...\n",
      "    (æ–°) ä½¿ç”¨æ‚¨æŒ‡å®šçš„ 7 ä¸ªè¿ç»­ç‰¹å¾: ['BitsAllocated', 'BitsStored', 'HighBit', 'LargestImagePixelValue', 'PixelRepresentation', 'SamplesPerPixel', 'SmallestImagePixelValue']\n",
      "    (æ–°) ä½¿ç”¨æ‚¨æŒ‡å®šçš„ 13 ä¸ªåˆ†ç±»ç‰¹å¾: ['left_or_right_breast', 'image_view', 'BodyPartExamined', 'ConversionType', 'Modality', 'PhotometricInterpretation', 'SecondaryCaptureDeviceManufacturer', 'SecondaryCaptureDeviceManufacturerModelName', 'SeriesDescription', 'SpecificCharacterSet', 'breast_density', 'assessment', 'subtlety']\n",
      "Step 3/11: æ£€æŸ¥å¹¶åˆ é™¤ 'æ’å®š' ç‰¹å¾...\n",
      "    -> æ ‡è®°ä¸ºåˆ é™¤ (Train å”¯ä¸€å€¼: 1, Test å”¯ä¸€å€¼: 1): BitsAllocated\n",
      "    -> æ ‡è®°ä¸ºåˆ é™¤ (Train å”¯ä¸€å€¼: 1, Test å”¯ä¸€å€¼: 1): BitsStored\n",
      "    -> æ ‡è®°ä¸ºåˆ é™¤ (Train å”¯ä¸€å€¼: 1, Test å”¯ä¸€å€¼: 1): HighBit\n",
      "    -> æ ‡è®°ä¸ºåˆ é™¤ (Train å”¯ä¸€å€¼: 1, Test å”¯ä¸€å€¼: 1): PixelRepresentation\n",
      "    -> æ ‡è®°ä¸ºåˆ é™¤ (Train å”¯ä¸€å€¼: 1, Test å”¯ä¸€å€¼: 1): SamplesPerPixel\n",
      "    -> æ ‡è®°ä¸ºåˆ é™¤ (Train å”¯ä¸€å€¼: 1, Test å”¯ä¸€å€¼: 1): SmallestImagePixelValue\n",
      "    -> æ ‡è®°ä¸ºåˆ é™¤ (Train å”¯ä¸€å€¼: 1, Test å”¯ä¸€å€¼: 1): BodyPartExamined\n",
      "    -> æ ‡è®°ä¸ºåˆ é™¤ (Train å”¯ä¸€å€¼: 1, Test å”¯ä¸€å€¼: 1): ConversionType\n",
      "    -> æ ‡è®°ä¸ºåˆ é™¤ (Train å”¯ä¸€å€¼: 1, Test å”¯ä¸€å€¼: 1): Modality\n",
      "    -> æ ‡è®°ä¸ºåˆ é™¤ (Train å”¯ä¸€å€¼: 1, Test å”¯ä¸€å€¼: 1): PhotometricInterpretation\n",
      "    -> æ ‡è®°ä¸ºåˆ é™¤ (Train å”¯ä¸€å€¼: 1, Test å”¯ä¸€å€¼: 1): SecondaryCaptureDeviceManufacturer\n",
      "    -> æ ‡è®°ä¸ºåˆ é™¤ (Train å”¯ä¸€å€¼: 1, Test å”¯ä¸€å€¼: 1): SecondaryCaptureDeviceManufacturerModelName\n",
      "    -> æ ‡è®°ä¸ºåˆ é™¤ (Train å”¯ä¸€å€¼: 1, Test å”¯ä¸€å€¼: 1): SeriesDescription\n",
      "    -> æ ‡è®°ä¸ºåˆ é™¤ (Train å”¯ä¸€å€¼: 1, Test å”¯ä¸€å€¼: 1): SpecificCharacterSet\n",
      "    âœ… å·²ä»ç‰¹å¾åˆ—è¡¨ä¸­ç§»é™¤ 14 ä¸ªæ’å®šåˆ—ã€‚\n",
      "    è·¯å¾„è½¬æ¢å®Œæˆã€‚ç¤ºä¾‹: /mnt/hdd/jiazy/cbis-ddsm-breast-cancer-image-dataset/jpeg/1.3.6.1.4.1.9590.100.1.2.148086015311392424203944628703986309919/1-064.jpg\n",
      "Step 4/11: è¿‡æ»¤æ— æ•ˆçš„æ ‡ç­¾/è·¯å¾„...\n",
      "    è¿‡æ»¤åå‰©ä½™: Train 2864, Test 422\n",
      "Step 5/11: å¡«å……ç‰¹å¾ä¸­çš„ç¼ºå¤±å€¼ (NaN)...\n",
      "-> æ­£åœ¨ä¸º _TIP (æ— å¡«è¡¥) ç‰ˆæœ¬åˆ›å»ºæ•°æ®å¤‡ä»½...\n",
      "Step 6/11: ç¼–ç åˆ†ç±»ç‰¹å¾...\n",
      "Step 6.B/11: ç¼–ç åˆ†ç±»ç‰¹å¾ [_TIP ç‰ˆ]...\n",
      "Step 7/11: åˆ’åˆ† Train/Val (80/20)ï¼ŒåŸºäº label_4c åˆ†å±‚...\n",
      "    Train: 2291, Val: 573, Test: 422\n",
      "Step 8/11: æ ‡å‡†åŒ–è¿ç»­ç‰¹å¾...\n",
      "Step 9/11: ä¿å­˜å¤„ç†åçš„ç‰¹å¾ CSV (æ— è¡¨å¤´) å’Œ field_lengths.pt...\n",
      "ğŸ’¾ Saved train_features.csv\n",
      "ğŸ’¾ Saved val_features.csv\n",
      "ğŸ’¾ Saved test_features.csv\n",
      "ğŸ’¾ Saved tabular_lengths.pt (Features: 6, Lengths: [1, 2, 2, 4, 6, 6])\n",
      "\n",
      "--- æ­£åœ¨å¤„ç†å¹¶ä¿å­˜ _TIP (æ— å¡«è¡¥) ç‰ˆæœ¬ ---\n",
      "    Step 7_TIP: åˆ’åˆ† _TIP æ•°æ® (ä½¿ç”¨ç›¸åŒç´¢å¼•)...\n",
      "    Step 8_TIP: æ ‡å‡†åŒ– _TIP æ•°æ® (é‡ç”¨ scaler)...\n",
      "    Step 9_TIP: ä¿å­˜ _TIP ç‰¹å¾ CSV...\n",
      "ğŸ’¾ Saved /mnt/hdd/jiazy/cbis-ddsm-breast-cancer-image-dataset/features/train_features_TIP.csv\n",
      "ğŸ’¾ Saved /mnt/hdd/jiazy/cbis-ddsm-breast-cancer-image-dataset/features/val_features_TIP.csv\n",
      "ğŸ’¾ Saved /mnt/hdd/jiazy/cbis-ddsm-breast-cancer-image-dataset/features/test_features_TIP.csv\n",
      "--- _TIP ç‰ˆæœ¬å¤„ç†å®Œæ¯• ---\n",
      "\n",
      "Step 10/11: ä¿å­˜ 4-class æ ‡ç­¾ .pt...\n",
      "ğŸ’¾ Saved /mnt/hdd/jiazy/cbis-ddsm-breast-cancer-image-dataset/features/train_labels.pt (Classes: [0 1 2 3])\n",
      "ğŸ’¾ Saved /mnt/hdd/jiazy/cbis-ddsm-breast-cancer-image-dataset/features/val_labels.pt (Classes: [0 1 2 3])\n",
      "ğŸ’¾ Saved /mnt/hdd/jiazy/cbis-ddsm-breast-cancer-image-dataset/features/test_labels.pt (Classes: [0 1 2 3])\n",
      "Step 11/11: è½¬æ¢å›¾åƒä¸º .npy å¹¶ä¿å­˜è·¯å¾„ .pt...\n",
      "\n",
      "æ­£åœ¨è½¬æ¢ train å›¾åƒ (.jpg -> .npy)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing train: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2291/2291 [00:00<00:00, 117113.35it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ’¾ Saved /mnt/hdd/jiazy/cbis-ddsm-breast-cancer-image-dataset/features/train_paths.pt (åŒ…å« 2291 æ¡ .npy è·¯å¾„)\n",
      "\n",
      "æ­£åœ¨è½¬æ¢ val å›¾åƒ (.jpg -> .npy)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing val: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 573/573 [00:00<00:00, 112658.14it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ’¾ Saved /mnt/hdd/jiazy/cbis-ddsm-breast-cancer-image-dataset/features/val_paths.pt (åŒ…å« 573 æ¡ .npy è·¯å¾„)\n",
      "\n",
      "æ­£åœ¨è½¬æ¢ test å›¾åƒ (.jpg -> .npy)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing test: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 422/422 [00:00<00:00, 109414.37it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ’¾ Saved /mnt/hdd/jiazy/cbis-ddsm-breast-cancer-image-dataset/features/test_paths.pt (åŒ…å« 422 æ¡ .npy è·¯å¾„)\n",
      "\n",
      "ğŸ‰ å…¨éƒ¨å®Œæˆ! å¤„ç†åçš„æ–‡ä»¶å·²å‡†å¤‡å°±ç»ª: /mnt/hdd/jiazy/cbis-ddsm-breast-cancer-image-dataset/features\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import torch\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "\n",
    "# ==============================\n",
    "# è·¯å¾„é…ç½®\n",
    "# ==============================\n",
    "# 1. åŒ…å« train_cleaned.csv å’Œ test_cleaned.csv çš„ç›®å½•\n",
    "DATASET_ROOT = \"/mnt/hdd/jiazy/cbis-ddsm-breast-cancer-image-dataset/\" \n",
    "BASE_IMAGE_PATH = \"/mnt/hdd/jiazy/\"\n",
    "# 2. å¤„ç†åæ–‡ä»¶çš„è¾“å‡ºç›®å½•\n",
    "OUT_DIR = os.path.join(DATASET_ROOT, \"features\")\n",
    "os.makedirs(OUT_DIR, exist_ok=True)\n",
    "\n",
    "# 3. (å·²æ›´æ–°) å®šä¹‰æ‚¨çš„è¾“å…¥æ–‡ä»¶\n",
    "TRAIN_CLEANED_CSV = os.path.join(DATASET_ROOT, \"train_cleaned.csv\")\n",
    "TEST_CLEANED_CSV = os.path.join(DATASET_ROOT, \"test_cleaned.csv\")\n",
    "\n",
    "print(f\"æ•°æ®æº (CSV): {DATASET_ROOT}\")\n",
    "print(f\"è¾“å‡ºç›®å½• (Processed): {OUT_DIR}\\n\")\n",
    "\n",
    "# ==============================\n",
    "# 1ï¸âƒ£ åŠ è½½å·²æ¸…ç†çš„ CSV\n",
    "# ==============================\n",
    "print(\"Step 1/11: åŠ è½½å·²æ¸…ç†çš„ train_cleaned.csv å’Œ test_cleaned.csv...\")\n",
    "if not os.path.exists(TRAIN_CLEANED_CSV) or not os.path.exists(TEST_CLEANED_CSV):\n",
    "    print(f\"ğŸ”´ é”™è¯¯ï¼šæ‰¾ä¸åˆ° {TRAIN_CLEANED_CSV} æˆ– {TEST_CLEANED_CSV}\")\n",
    "    print(\"è¯·ç¡®ä¿ DATASET_ROOT å˜é‡è®¾ç½®æ­£ç¡®ã€‚\")\n",
    "    exit(1)\n",
    "\n",
    "try:\n",
    "    train_df = pd.read_csv(TRAIN_CLEANED_CSV)\n",
    "    test_df  = pd.read_csv(TEST_CLEANED_CSV)\n",
    "except Exception as e:\n",
    "    print(f\"ğŸ”´ é”™è¯¯ï¼šè¯»å– CSV å¤±è´¥: {e}\")\n",
    "    exit(1)\n",
    "\n",
    "print(f\"    Train DF (loaded): {len(train_df)} è¡Œ, Test DF (loaded): {len(test_df)} è¡Œ\")\n",
    "\n",
    "# ==============================\n",
    "# 2ï¸âƒ£ (å·²ä¿®æ”¹) å®šä¹‰ç‰¹å¾ç±»å‹ (åŸºäºæ‚¨çš„è¦æ±‚)\n",
    "# ==============================\n",
    "print(\"Step 2/11: å®šä¹‰ç‰¹å¾ç±»å‹ (åŸºäºæ‚¨çš„è¦æ±‚)...\")\n",
    "new_label_col = \"label_4c\"\n",
    "image_path_col_final = \"image_path_final\" \n",
    "\n",
    "# 1. æ¥è‡ªæ‚¨è¦æ±‚çš„éç‰¹å¾åˆ—\n",
    "# (æˆ‘ä»¬åˆå¹¶äº†æ‚¨æä¾›çš„åˆ—å’Œè„šæœ¬ä¸­åŸæœ‰çš„å…¶ä»–éç‰¹å¾åˆ—)\n",
    "non_feature_cols_from_script = [\n",
    "    \"pathology\", \"abnormality type\", \"lesion_type\",\n",
    "    \"patient_id\", \"file_path\", \"abnormality id\"\n",
    "]\n",
    "non_feature_cols = list(set(\n",
    "    [new_label_col, image_path_col_final] + non_feature_cols_from_script\n",
    "))\n",
    "\n",
    "# 2. æ¥è‡ªæ‚¨è¦æ±‚çš„è¿ç»­ç‰¹å¾\n",
    "final_continuous_cols = [\n",
    "    'BitsAllocated', 'BitsStored', 'HighBit', 'LargestImagePixelValue', \n",
    "    'PixelRepresentation', 'SamplesPerPixel', 'SmallestImagePixelValue'\n",
    "]\n",
    "\n",
    "# 3. æ¥è‡ªæ‚¨è¦æ±‚çš„ç±»åˆ«ç‰¹å¾\n",
    "final_categorical_cols = [\n",
    "    'left_or_right_breast', 'image_view', 'BodyPartExamined', 'ConversionType', \n",
    "    'Modality', 'PhotometricInterpretation', 'SecondaryCaptureDeviceManufacturer', \n",
    "    'SecondaryCaptureDeviceManufacturerModelName', 'SeriesDescription', \n",
    "    'SpecificCharacterSet', 'breast_density', 'assessment', 'subtlety'\n",
    "]\n",
    "\n",
    "# ç¡®ä¿è¿™äº›åˆ—å­˜åœ¨äº DataFrame ä¸­ï¼Œè¿‡æ»¤æ‰ä¸å­˜åœ¨çš„åˆ—\n",
    "all_feature_cols_requested = final_continuous_cols + final_categorical_cols\n",
    "available_cols = set(train_df.columns)\n",
    "\n",
    "final_continuous_cols = [col for col in final_continuous_cols if col in available_cols]\n",
    "final_categorical_cols = [col for col in final_categorical_cols if col in available_cols]\n",
    "\n",
    "missing_cols = [col for col in all_feature_cols_requested if col not in available_cols]\n",
    "if missing_cols:\n",
    "    print(f\"    âš ï¸ è­¦å‘Š: æ‚¨æŒ‡å®šçš„ä¸€äº›ç‰¹å¾åˆ—åœ¨ train_df.csv ä¸­ä¸å­˜åœ¨ï¼Œå°†è¢«å¿½ç•¥: {missing_cols}\")\n",
    "\n",
    "print(f\"    (æ–°) ä½¿ç”¨æ‚¨æŒ‡å®šçš„ {len(final_continuous_cols)} ä¸ªè¿ç»­ç‰¹å¾: {final_continuous_cols}\")\n",
    "print(f\"    (æ–°) ä½¿ç”¨æ‚¨æŒ‡å®šçš„ {len(final_categorical_cols)} ä¸ªåˆ†ç±»ç‰¹å¾: {final_categorical_cols}\")\n",
    "\n",
    "# ==============================\n",
    "# 3ï¸âƒ£ (æ–°) åˆ é™¤åœ¨ train/test ä¸­å‡åªæœ‰ä¸€ä¸ªå”¯ä¸€å€¼çš„åˆ—\n",
    "# ==============================\n",
    "print(\"Step 3/11: æ£€æŸ¥å¹¶åˆ é™¤ 'æ’å®š' ç‰¹å¾...\")\n",
    "cols_to_drop = []\n",
    "all_feature_cols = final_continuous_cols + final_categorical_cols\n",
    "\n",
    "for col in all_feature_cols:\n",
    "    # æ£€æŸ¥åˆ—æ˜¯å¦å­˜åœ¨ï¼Œä»¥é˜²ä¸‡ä¸€\n",
    "    if col not in train_df.columns or col not in test_df.columns:\n",
    "        continue\n",
    "        \n",
    "    try:\n",
    "        # nunique() é»˜è®¤ä¸è®¡ç®— NaN\n",
    "        train_unique_count = train_df[col].nunique()\n",
    "        test_unique_count = test_df[col].nunique()\n",
    "        \n",
    "        # å¦‚æœåœ¨ train å’Œ test ä¸­ï¼Œå”¯ä¸€å€¼çš„æ•°é‡éƒ½ä¸º 1 (æˆ– 0)\n",
    "        if train_unique_count <= 1 and test_unique_count <= 1:\n",
    "            cols_to_drop.append(col)\n",
    "            print(f\"    -> æ ‡è®°ä¸ºåˆ é™¤ (Train å”¯ä¸€å€¼: {train_unique_count}, Test å”¯ä¸€å€¼: {test_unique_count}): {col}\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"    âš ï¸ æ£€æŸ¥ {col} å”¯ä¸€å€¼æ—¶å‡ºé”™: {e}\")\n",
    "\n",
    "# ä»ç‰¹å¾åˆ—è¡¨ä¸­ç§»é™¤è¿™äº›åˆ—\n",
    "final_continuous_cols = [col for col in final_continuous_cols if col not in cols_to_drop]\n",
    "final_categorical_cols = [col for col in final_categorical_cols if col not in cols_to_drop]\n",
    "\n",
    "if cols_to_drop:\n",
    "    print(f\"    âœ… å·²ä»ç‰¹å¾åˆ—è¡¨ä¸­ç§»é™¤ {len(cols_to_drop)} ä¸ªæ’å®šåˆ—ã€‚\")\n",
    "else:\n",
    "    print(\"    âœ… æœªå‘ç°æ’å®šç‰¹å¾åˆ—ã€‚\")\n",
    "\n",
    "def make_abs_path(path):\n",
    "    # å¦‚æœè·¯å¾„æ˜¯ NaN æˆ– Noneï¼Œä¿æŒåŸæ ·\n",
    "    if pd.isna(path):\n",
    "        return path\n",
    "    # ä½¿ç”¨ os.path.join æ™ºèƒ½åœ°æ‹¼æ¥åŸºç¡€è·¯å¾„å’Œç›¸å¯¹è·¯å¾„\n",
    "    return os.path.join(BASE_IMAGE_PATH, str(path))\n",
    "\n",
    "train_df[image_path_col_final] = train_df[image_path_col_final].apply(make_abs_path)\n",
    "test_df[image_path_col_final] = test_df[image_path_col_final].apply(make_abs_path)\n",
    "\n",
    "print(f\"    è·¯å¾„è½¬æ¢å®Œæˆã€‚ç¤ºä¾‹: {train_df[image_path_col_final].iloc[0]}\")\n",
    "\n",
    "# ==============================\n",
    "# 4ï¸âƒ£ (åŸ Step 3) è¿‡æ»¤æ ‡ç­¾/è·¯å¾„\n",
    "# ==============================\n",
    "print(\"Step 4/11: è¿‡æ»¤æ— æ•ˆçš„æ ‡ç­¾/è·¯å¾„...\")\n",
    "# (æ‚¨çš„ info æ˜¾ç¤º 2864 non-nullï¼Œä½†è¿™ä¸€æ­¥æ˜¯ç¡®ä¿å®‰å…¨çš„)\n",
    "train_df.dropna(subset=[image_path_col_final, new_label_col], inplace=True)\n",
    "test_df.dropna(subset=[image_path_col_final, new_label_col], inplace=True)\n",
    "print(f\"    è¿‡æ»¤åå‰©ä½™: Train {len(train_df)}, Test {len(test_df)}\")\n",
    "\n",
    "# ==============================\n",
    "# 5ï¸âƒ£ (åŸ Step 4) å¡«å……ç‰¹å¾ä¸­çš„ç¼ºå¤±å€¼ (NaN)\n",
    "# ==============================\n",
    "print(\"Step 5/11: å¡«å……ç‰¹å¾ä¸­çš„ç¼ºå¤±å€¼ (NaN)...\")\n",
    "# (ä½¿ç”¨ .loc é¿å… FutureWarning)\n",
    "print(\"-> æ­£åœ¨ä¸º _TIP (æ— å¡«è¡¥) ç‰ˆæœ¬åˆ›å»ºæ•°æ®å¤‡ä»½...\")\n",
    "train_df_tip = train_df.copy()\n",
    "test_df_tip = test_df.copy()\n",
    "\n",
    "\n",
    "for col in final_continuous_cols:\n",
    "    mean_val = train_df[col].mean()\n",
    "    train_df.loc[:, col] = train_df[col].fillna(mean_val)\n",
    "    test_df.loc[:, col]  = test_df[col].fillna(mean_val)\n",
    "\n",
    "for col in final_categorical_cols:\n",
    "    train_df.loc[:, col] = train_df[col].fillna(\"MISSING\")\n",
    "    test_df.loc[:, col]  = test_df[col].fillna(\"MISSING\")\n",
    "\n",
    "# ==============================\n",
    "# 6ï¸âƒ£ (åŸ Step 5) ç¼–ç åˆ†ç±»ç‰¹å¾\n",
    "# ==============================\n",
    "print(\"Step 6/11: ç¼–ç åˆ†ç±»ç‰¹å¾...\")\n",
    "field_lengths = [] \n",
    "# æ£€æŸ¥ï¼šå¦‚æœåˆ é™¤äº†æ‰€æœ‰åˆ†ç±»åˆ—ï¼Œåˆ™è·³è¿‡\n",
    "if final_categorical_cols:\n",
    "    full_df_imputed = pd.concat([train_df, test_df], ignore_index=True)\n",
    "\n",
    "    for col in final_categorical_cols:\n",
    "        full_df_imputed[col] = full_df_imputed[col].astype('category')\n",
    "        codes = full_df_imputed[col].cat.codes\n",
    "        cardinality = len(full_df_imputed[col].cat.categories)\n",
    "        field_lengths.append(cardinality)\n",
    "        train_df[col] = codes.iloc[:len(train_df)].values\n",
    "        test_df[col]  = codes.iloc[len(train_df):].values\n",
    "else:\n",
    "    print(\"    (è·³è¿‡ï¼šæœªæ‰¾åˆ°åˆ†ç±»ç‰¹å¾)\")\n",
    "\n",
    "print(\"Step 6.B/11: ç¼–ç åˆ†ç±»ç‰¹å¾ [_TIP ç‰ˆ]...\")\n",
    "if final_categorical_cols:\n",
    "    full_df_tip = pd.concat([train_df_tip, test_df_tip], ignore_index=True)\n",
    "    for col in final_categorical_cols:\n",
    "        full_df_tip[col] = full_df_tip[col].astype('category')\n",
    "        # .cat.codes ä¼šè‡ªåŠ¨å°† NaN æ˜ å°„ä¸º -1\n",
    "        codes_tip = full_df_tip[col].cat.codes\n",
    "        # (æˆ‘ä»¬ä¸éœ€è¦ä¸º TIP ç‰ˆæœ¬ä¿å­˜ field_lengths)\n",
    "        train_df_tip[col] = codes_tip.iloc[:len(train_df_tip)].values\n",
    "        test_df_tip[col]  = codes_tip.iloc[len(train_df_tip):].values\n",
    "# --- [æ–°å¢ç»“æŸ] ---\n",
    "\n",
    "# ==============================\n",
    "# 7ï¸âƒ£ (åŸ Step 6) åˆ’åˆ† train / val\n",
    "# ==============================\n",
    "print(f\"Step 7/11: åˆ’åˆ† Train/Val (80/20)ï¼ŒåŸºäº {new_label_col} åˆ†å±‚...\")\n",
    "\n",
    "if len(train_df) == 0:\n",
    "    print(\"ğŸ”´ é”™è¯¯ï¼štrain_df åœ¨è¿‡æ»¤åä¸ºç©ºï¼æ— æ³•è¿›è¡Œ train_test_splitã€‚\")\n",
    "    print(\"    è¯·æ£€æŸ¥æ‚¨çš„ train_cleaned.csv æ–‡ä»¶æ˜¯å¦åŒ…å«æœ‰æ•ˆçš„ 'image_path_final' å’Œ 'label_4c'ã€‚\")\n",
    "    exit(1)\n",
    "\n",
    "train_split, val_split = train_test_split(\n",
    "    train_df, test_size=0.2, random_state=42,\n",
    "    stratify=train_df[new_label_col] \n",
    ")\n",
    "test_split = test_df # test_df ä¿æŒä¸º test_cleaned.csv çš„å…¨éƒ¨å†…å®¹\n",
    "print(f\"    Train: {len(train_split)}, Val: {len(val_split)}, Test: {len(test_split)}\")\n",
    "train_indices = train_split.index\n",
    "val_indices = val_split.index\n",
    "\n",
    "# ==============================\n",
    "# 8ï¸âƒ£ (åŸ Step 7) æ ‡å‡†åŒ–è¿ç»­ç‰¹å¾\n",
    "# ==============================\n",
    "print(\"Step 8/11: æ ‡å‡†åŒ–è¿ç»­ç‰¹å¾...\")\n",
    "scaler = StandardScaler()\n",
    "if final_continuous_cols: \n",
    "    scaler.fit(train_split[final_continuous_cols])\n",
    "    # (ä½¿ç”¨ .loc é¿å… FutureWarning)\n",
    "    train_split.loc[:, final_continuous_cols] = scaler.transform(train_split[final_continuous_cols])\n",
    "    val_split.loc[:, final_continuous_cols]   = scaler.transform(val_split[final_continuous_cols])\n",
    "    test_split.loc[:, final_continuous_cols]  = scaler.transform(test_split[final_continuous_cols])\n",
    "else:\n",
    "    print(\"    (è·³è¿‡ï¼šæœªæ‰¾åˆ°è¿ç»­ç‰¹å¾)\")\n",
    "\n",
    "# ==============================\n",
    "# 9ï¸âƒ£ (åŸ Step 8) ä¿å­˜å¤„ç†åçš„ç‰¹å¾ & å­—æ®µé•¿åº¦\n",
    "# ==============================\n",
    "print(\"Step 9/11: ä¿å­˜å¤„ç†åçš„ç‰¹å¾ CSV (æ— è¡¨å¤´) å’Œ field_lengths.pt...\")\n",
    "# DVM-Car è„šæœ¬é¡ºåº: è¿ç»­å‹, ç„¶å åˆ†ç±»å‹\n",
    "ordered_feature_cols = final_continuous_cols + final_categorical_cols\n",
    "# ä¸ºè¿ç»­ç‰¹å¾æ·»åŠ  '1' åˆ° field_lengths çš„å¼€å¤´\n",
    "# (field_lengths ä»…åŒ…å«åˆ†ç±»ç‰¹å¾çš„åŸºæ•°)\n",
    "final_field_lengths = [1] * len(final_continuous_cols) + field_lengths\n",
    "\n",
    "for df, name in zip([train_split, val_split, test_split], [\"train\", \"val\", \"test\"]):\n",
    "    # ä»…å½“ ordered_feature_cols ä¸ä¸ºç©ºæ—¶æ‰ä¿å­˜\n",
    "    if ordered_feature_cols:\n",
    "        features_only_df = df[ordered_feature_cols]\n",
    "        # å¯¹åº”æ‚¨åˆ—è¡¨ä¸­çš„ dvm_features_..._physical_jittered_50.csv\n",
    "        out_path = os.path.join(OUT_DIR, f\"{name}_features.csv\")\n",
    "        features_only_df.to_csv(out_path, index=False, header=False) \n",
    "        print(f\"ğŸ’¾ Saved {name}_features.csv\")\n",
    "    else:\n",
    "        print(f\"    (è·³è¿‡ {name}_features.csvï¼šæœªæ‰¾åˆ°ä»»ä½•ç‰¹å¾)\")\n",
    "\n",
    "# å¯¹åº”æ‚¨åˆ—è¡¨ä¸­çš„ tabular_lengths_all_views_physical.pt\n",
    "torch.save(final_field_lengths, os.path.join(OUT_DIR, \"tabular_lengths.pt\"))\n",
    "print(f\"ğŸ’¾ Saved tabular_lengths.pt (Features: {len(final_field_lengths)}, Lengths: {final_field_lengths})\")\n",
    "\n",
    "# ==============================\n",
    "# 9ï¸âƒ£.B [æ–°å¢] å¤„ç†å¹¶ä¿å­˜ _TIP ç‰ˆæœ¬\n",
    "# ==============================\n",
    "print(\"\\n--- æ­£åœ¨å¤„ç†å¹¶ä¿å­˜ _TIP (æ— å¡«è¡¥) ç‰ˆæœ¬ ---\")\n",
    "\n",
    "# 1. [TIP åˆ’åˆ†] ä½¿ç”¨ \"å¡«è¡¥ç‰ˆ\" çš„ç´¢å¼•æ¥åˆ’åˆ† \"TIPç‰ˆ\" æ•°æ®\n",
    "print(\"    Step 7_TIP: åˆ’åˆ† _TIP æ•°æ® (ä½¿ç”¨ç›¸åŒç´¢å¼•)...\")\n",
    "train_split_tip = train_df_tip.loc[train_indices]\n",
    "val_split_tip = train_df_tip.loc[val_indices]\n",
    "test_split_tip = test_df_tip # Test é›†ä¿æŒå®Œæ•´\n",
    "\n",
    "# 2. [TIP æ ‡å‡†åŒ–] é‡ç”¨ä¹‹å‰ fit å¥½çš„ scaler æ¥ transform\n",
    "#    (StandardScaler ä¼šè‡ªåŠ¨ä¼ æ’­ NaNsï¼Œè¿™æ­£æ˜¯æˆ‘ä»¬æƒ³è¦çš„)\n",
    "print(\"    Step 8_TIP: æ ‡å‡†åŒ– _TIP æ•°æ® (é‡ç”¨ scaler)...\")\n",
    "if final_continuous_cols: \n",
    "    train_split_tip.loc[:, final_continuous_cols] = scaler.transform(train_split_tip[final_continuous_cols])\n",
    "    val_split_tip.loc[:, final_continuous_cols]   = scaler.transform(val_split_tip[final_continuous_cols])\n",
    "    test_split_tip.loc[:, final_continuous_cols]  = scaler.transform(test_split_tip[final_continuous_cols])\n",
    "\n",
    "# 3. [TIP ä¿å­˜] ä¿å­˜å¸¦ _TIP åç¼€çš„ CSV\n",
    "print(\"    Step 9_TIP: ä¿å­˜ _TIP ç‰¹å¾ CSV...\")\n",
    "for df, name in zip([train_split_tip, val_split_tip, test_split_tip], [\"train\", \"val\", \"test\"]):\n",
    "    if ordered_feature_cols:\n",
    "        features_only_df_tip = df[ordered_feature_cols]\n",
    "        # *** [ä¿®æ”¹] æ·»åŠ  _TIP åç¼€ ***\n",
    "        out_path_tip = os.path.join(OUT_DIR, f\"{name}_features_TIP.csv\")\n",
    "        features_only_df_tip.to_csv(out_path_tip, index=False, header=False) \n",
    "        print(f\"ğŸ’¾ Saved {out_path_tip}\")\n",
    "    else:\n",
    "        print(f\"    (è·³è¿‡ {name}_features_TIP.csvï¼šæœªæ‰¾åˆ°ä»»ä½•ç‰¹å¾)\")\n",
    "print(\"--- _TIP ç‰ˆæœ¬å¤„ç†å®Œæ¯• ---\\n\")\n",
    "\n",
    "\n",
    "# ==============================\n",
    "# ğŸ”Ÿ (åŸ Step 9) ä¿å­˜æ ‡ç­¾ .pt æ–‡ä»¶\n",
    "# ==============================\n",
    "print(\"Step 10/11: ä¿å­˜ 4-class æ ‡ç­¾ .pt...\")\n",
    "def save_labels_4c(df, name):\n",
    "    labels = df[new_label_col].astype(int).values\n",
    "    # å¯¹åº”æ‚¨åˆ—è¡¨ä¸­çš„ labels_model_all_{name}_all_views.pt\n",
    "    out_path = os.path.join(OUT_DIR, f\"{name}_labels.pt\")\n",
    "    torch.save(torch.tensor(labels, dtype=torch.long), out_path)\n",
    "    print(f\"ğŸ’¾ Saved {out_path} (Classes: {np.unique(labels)})\")\n",
    "\n",
    "save_labels_4c(train_split, \"train\")\n",
    "save_labels_4c(val_split, \"val\")\n",
    "save_labels_4c(test_split, \"test\")\n",
    "\n",
    "# ==============================\n",
    "# 1ï¸âƒ£1ï¸âƒ£ (åŸ Step 10) è½¬æ¢å›¾åƒä¸º .NPY å¹¶ä¿å­˜ .NPY è·¯å¾„\n",
    "# ==============================\n",
    "print(\"Step 11/11: è½¬æ¢å›¾åƒä¸º .npy å¹¶ä¿å­˜è·¯å¾„ .pt...\")\n",
    "\n",
    "def convert_and_save_paths(df, name):\n",
    "    # å‡è®¾ 'image_path_final' æ˜¯ç»å¯¹ JPG è·¯å¾„\n",
    "    original_jpg_paths = df[image_path_col_final].tolist()\n",
    "    npy_paths = [] \n",
    "    \n",
    "    print(f\"\\næ­£åœ¨è½¬æ¢ {name} å›¾åƒ (.jpg -> .npy)...\")\n",
    "    for img_path in tqdm(original_jpg_paths, desc=f\"Processing {name}\"):\n",
    "        if img_path is None or pd.isna(img_path):\n",
    "            continue\n",
    "            \n",
    "        if not os.path.exists(img_path):\n",
    "            print(f\"âš ï¸ è·¯å¾„ä¸å­˜åœ¨ï¼Œè·³è¿‡: {img_path}\")\n",
    "            continue\n",
    "            \n",
    "        # .npy æ–‡ä»¶å°†è¢«ä¿å­˜åœ¨ .jpg æ—è¾¹\n",
    "        save_path = os.path.splitext(img_path)[0] + \".npy\"\n",
    "        \n",
    "        try:\n",
    "            if not os.path.exists(save_path):\n",
    "                img = Image.open(img_path)\n",
    "                img_np = np.array(img)\n",
    "                # ç¡®ä¿ç›®å½•å­˜åœ¨ (ä»¥é˜²ä¸‡ä¸€)\n",
    "                os.makedirs(os.path.dirname(save_path), exist_ok=True)\n",
    "                np.save(save_path, img_np)\n",
    "            \n",
    "            npy_paths.append(save_path)\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {img_path}: {e}\")\n",
    "    \n",
    "    # å¯¹åº”æ‚¨åˆ—è¡¨ä¸­çš„ {name}_paths_all_views.pt\n",
    "    out_path = os.path.join(OUT_DIR, f\"{name}_paths.pt\")\n",
    "    torch.save(npy_paths, out_path)\n",
    "    print(f\"ğŸ’¾ Saved {out_path} (åŒ…å« {len(npy_paths)} æ¡ .npy è·¯å¾„)\")\n",
    "\n",
    "convert_and_save_paths(train_split, \"train\")\n",
    "convert_and_save_paths(val_split, \"val\")\n",
    "convert_and_save_paths(test_split, \"test\")\n",
    "\n",
    "print(f\"\\nğŸ‰ å…¨éƒ¨å®Œæˆ! å¤„ç†åçš„æ–‡ä»¶å·²å‡†å¤‡å°±ç»ª: {OUT_DIR}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fad8dfca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- æ­£åœ¨æ£€æŸ¥å›¾åƒæ¨¡å¼ ---\n",
      "æˆåŠŸåŠ è½½: /mnt/hdd/jiazy/cbis-ddsm-breast-cancer-image-dataset/train_cleaned.csv\n",
      "\n",
      "==============================\n",
      " ğŸ“Š å›¾åƒè¯Šæ–­æŠ¥å‘Š (ç¬¬ä¸€å¼ å›¾)\n",
      "==============================\n",
      " æ–‡ä»¶è·¯å¾„: /mnt/hdd/jiazy/cbis-ddsm-breast-cancer-image-dataset/jpeg/1.3.6.1.4.1.9590.100.1.2.148086015311392424203944628703986309919/1-064.jpg\n",
      " å›¾åƒå°ºå¯¸: (3800, 5640)\n",
      " å›¾åƒæ¨¡å¼: L\n",
      "==============================\n",
      "âœ… ç»“è®ºï¼šè¿™æ˜¯ 'L' æ¨¡å¼ï¼Œå³**ç°åº¦å›¾ (Grayscale)**ã€‚\n",
      "   (æ‚¨åº”è¯¥åœ¨ transform ä¸­ä½¿ç”¨ A.ToRGB() å°†å…¶è½¬ä¸º 3 é€šé“)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "\n",
    "# --- 1. é…ç½® (ä»æ‚¨çš„é¢„å¤„ç†è„šæœ¬ä¸­å¤åˆ¶) ---\n",
    "\n",
    "# !! ç¡®ä¿è¿™äº›è·¯å¾„ä¸æ‚¨çš„ç¯å¢ƒä¸€è‡´ !!\n",
    "DATASET_ROOT = \"/mnt/hdd/jiazy/cbis-ddsm-breast-cancer-image-dataset/\" \n",
    "BASE_IMAGE_PATH = \"/mnt/hdd/jiazy/\"\n",
    "TRAIN_CLEANED_CSV = os.path.join(DATASET_ROOT, \"train_cleaned.csv\")\n",
    "image_path_col_final = \"image_path_final\" \n",
    "\n",
    "# --- 2. è¾…åŠ©å‡½æ•° (ä»æ‚¨çš„é¢„å¤„ç†è„šæœ¬ä¸­å¤åˆ¶) ---\n",
    "\n",
    "def make_abs_path(path):\n",
    "    \"\"\"æ„å»ºç»å¯¹è·¯å¾„\"\"\"\n",
    "    if pd.isna(path):\n",
    "        return path\n",
    "    # ä½¿ç”¨ os.path.join æ™ºèƒ½åœ°æ‹¼æ¥åŸºç¡€è·¯å¾„å’Œç›¸å¯¹è·¯å¾„\n",
    "    return os.path.join(BASE_IMAGE_PATH, str(path))\n",
    "\n",
    "# --- 3. ä¸»æ£€æŸ¥é€»è¾‘ ---\n",
    "\n",
    "def check_first_image_mode():\n",
    "    print(f\"--- æ­£åœ¨æ£€æŸ¥å›¾åƒæ¨¡å¼ ---\")\n",
    "    \n",
    "    # 1. åŠ è½½ CSV\n",
    "    try:\n",
    "        df = pd.read_csv(TRAIN_CLEANED_CSV)\n",
    "        print(f\"æˆåŠŸåŠ è½½: {TRAIN_CLEANED_CSV}\")\n",
    "    except FileNotFoundError:\n",
    "        print(f\"âŒ é”™è¯¯ï¼šæ‰¾ä¸åˆ° CSV æ–‡ä»¶ã€‚è¯·æ£€æŸ¥è·¯å¾„ï¼š\")\n",
    "        print(f\"{TRAIN_CLEANED_CSV}\")\n",
    "        return\n",
    "    except Exception as e:\n",
    "        print(f\"âŒ é”™è¯¯ï¼šåŠ è½½ CSV æ—¶å‡ºé”™: {e}\")\n",
    "        return\n",
    "\n",
    "    # 2. æ„å»ºç»å¯¹è·¯å¾„\n",
    "    df[image_path_col_final] = df[image_path_col_final].apply(make_abs_path)\n",
    "    \n",
    "    # 3. è¿‡æ»¤æ‰æ— æ•ˆè·¯å¾„\n",
    "    df = df.dropna(subset=[image_path_col_final])\n",
    "    if len(df) == 0:\n",
    "        print(\"âŒ é”™è¯¯ï¼šåœ¨ CSV ä¸­æ‰¾ä¸åˆ°ä»»ä½•æœ‰æ•ˆçš„å›¾åƒè·¯å¾„ã€‚\")\n",
    "        return\n",
    "\n",
    "    # 4. è·å–ç¬¬ä¸€å¼ å›¾ç‰‡çš„è·¯å¾„\n",
    "    first_image_path = df[image_path_col_final].iloc[0]\n",
    "    \n",
    "    if not os.path.exists(first_image_path):\n",
    "        print(f\"âŒ é”™è¯¯ï¼šæ‰¾åˆ°è·¯å¾„ä½†æ–‡ä»¶ä¸å­˜åœ¨: {first_image_path}\")\n",
    "        print(\"è¯·ç¡®ä¿ BASE_IMAGE_PATH è®¾ç½®æ­£ç¡®ã€‚\")\n",
    "        return\n",
    "\n",
    "    # 5. æ‰“å¼€å›¾åƒå¹¶æ£€æŸ¥æ¨¡å¼\n",
    "    try:\n",
    "        with Image.open(first_image_path) as img:\n",
    "            mode = img.mode\n",
    "            size = img.size\n",
    "        \n",
    "        print(\"\\n\" + \"=\"*30)\n",
    "        print(\" ğŸ“Š å›¾åƒè¯Šæ–­æŠ¥å‘Š (ç¬¬ä¸€å¼ å›¾)\")\n",
    "        print(\"=\"*30)\n",
    "        print(f\" æ–‡ä»¶è·¯å¾„: {first_image_path}\")\n",
    "        print(f\" å›¾åƒå°ºå¯¸: {size}\")\n",
    "        print(f\" å›¾åƒæ¨¡å¼: {mode}\")\n",
    "        print(\"=\"*30)\n",
    "\n",
    "        # 6. è§£é‡Šç»“æœ\n",
    "        if mode == 'L':\n",
    "            print(\"âœ… ç»“è®ºï¼šè¿™æ˜¯ 'L' æ¨¡å¼ï¼Œå³**ç°åº¦å›¾ (Grayscale)**ã€‚\")\n",
    "            print(\"   (æ‚¨åº”è¯¥åœ¨ transform ä¸­ä½¿ç”¨ A.ToRGB() å°†å…¶è½¬ä¸º 3 é€šé“)\")\n",
    "        elif mode == 'RGB':\n",
    "            print(\"âœ… ç»“è®ºï¼šè¿™æ˜¯ 'RGB' æ¨¡å¼ï¼Œå³**å½©è‰²å›¾ (3 é€šé“)**ã€‚\")\n",
    "            print(\"   (æ‚¨ä¸éœ€è¦ A.ToRGB()ï¼Œä½†ä½¿ç”¨å®ƒä¹Ÿæ˜¯å®‰å…¨çš„)\")\n",
    "        elif mode == 'RGBA':\n",
    "            print(\"âœ… ç»“è®ºï¼šè¿™æ˜¯ 'RGBA' æ¨¡å¼ (å¸¦é€æ˜åº¦)ã€‚\")\n",
    "            print(\"   (A.ToRGB() ä¼šè‡ªåŠ¨å¤„ç†å®ƒ)\")\n",
    "        else:\n",
    "            print(f\"âš ï¸ ç»“è®ºï¼šè¿™æ˜¯ä¸€ä¸ªä¸å¸¸è§çš„æ¨¡å¼ ('{mode}')ã€‚\")\n",
    "            print(\"   (å¼ºçƒˆå»ºè®®ä½¿ç”¨ A.ToRGB() æ¥æ ‡å‡†åŒ–)\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"âŒ é”™è¯¯ï¼šæ‰“å¼€æˆ–æ£€æŸ¥å›¾åƒæ—¶å‡ºé”™: {e}\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    check_first_image_mode()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "charms",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
