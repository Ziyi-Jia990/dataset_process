{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e0789496",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "å¼€å§‹æ‰§è¡Œ PetFinder æ•°æ®é›†é¢„å¤„ç†æµç¨‹...\n",
      "æ­£åœ¨åŠ è½½æ•°æ® /mnt/hdd/jiazy/petfinder-pawpularity-score/train.csv...\n",
      "æˆåŠŸåŠ è½½ 9912 æ¡è®°å½•ã€‚\n",
      "æ­£åœ¨æ„é€ å›¾åƒè·¯å¾„...\n",
      "æ­£åœ¨å®šä¹‰ç‰¹å¾...\n",
      "æ­£åœ¨æ¸…æ´—æ•°æ®...\n",
      "æ­£åœ¨è¿›è¡Œç±»åˆ«ç‰¹å¾ç¼–ç ...\n",
      "æ­£åœ¨è¿›è¡Œ 80:10:10 æ•°æ®é›†åˆ’åˆ†...\n",
      "åˆ’åˆ†ç»“æœ: è®­ç»ƒé›† 7929, éªŒè¯é›† 991, æµ‹è¯•é›† 992\n",
      "æ­£åœ¨å¤„ç†å›¾åƒ (Resize & Save .npy)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train Images: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 7929/7929 [04:07<00:00, 31.98it/s]\n",
      "Val Images: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 991/991 [00:30<00:00, 32.99it/s]\n",
      "Test Images: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 992/992 [00:31<00:00, 31.56it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Final] æ­£åœ¨ä¿å­˜å¤„ç†åçš„æ–‡ä»¶åˆ° ./features ...\n",
      "é¢„å¤„ç†å…¨éƒ¨å®Œæˆï¼\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "\n",
    "# ç¡®ä¿ PIL ç‰ˆæœ¬å…¼å®¹ (ç”¨äº resize)\n",
    "try:\n",
    "    # PIL 9.0.0+\n",
    "    LANCZOS_RESAMPLE = Image.Resampling.LANCZOS\n",
    "except AttributeError:\n",
    "    # Older PIL\n",
    "    LANCZOS_RESAMPLE = Image.LANCZOS\n",
    "\n",
    "# --- 1. å®šä¹‰å¸¸é‡å’Œè·¯å¾„ ---\n",
    "# è¯·æ ¹æ®æ‚¨çš„ç¯å¢ƒä¿®æ”¹è¿™é‡Œçš„æ ¹è·¯å¾„\n",
    "DATA_ROOT = \"/mnt/hdd/jiazy/petfinder-pawpularity-score\"\n",
    "METADATA_FILE = os.path.join(DATA_ROOT, \"train.csv\")\n",
    "IMAGE_DIR = os.path.join(DATA_ROOT, \"train\")\n",
    "\n",
    "# è¾“å‡ºç›®å½•\n",
    "OUTPUT_DIR = \"./features\"\n",
    "if not os.path.exists(OUTPUT_DIR):\n",
    "    os.makedirs(OUTPUT_DIR)\n",
    "\n",
    "# --- 2. å®šä¹‰ç‰¹å¾åˆ— ---\n",
    "# è¿ç»­ç‰¹å¾\n",
    "CONTINUOUS_COLS = []\n",
    "\n",
    "# ç±»åˆ«ç‰¹å¾\n",
    "CATEGORICAL_COLS = [\n",
    "    'Subject Focus', 'Eyes', 'Face', 'Near', 'Action', 'Accessory', 'Group', 'Collage'\n",
    "]\n",
    "\n",
    "# æ ‡ç­¾åˆ—\n",
    "LABEL_COL = 'Pawpularity'\n",
    "\n",
    "# éç‰¹å¾åˆ— (å°†è¢«ç§»é™¤)\n",
    "NON_FEATURE_COLS = [\n",
    "    'Id', \n",
    "]\n",
    "\n",
    "# --- 3. å›¾åƒå¤„ç†è¾…åŠ©å‡½æ•° ---\n",
    "def process_and_save_image(png_path):\n",
    "    \"\"\"\n",
    "    åŠ è½½ PNG å›¾åƒ, resize åˆ° 224x224, å¹¶å¦å­˜ä¸º .npy æ–‡ä»¶ã€‚\n",
    "    å¦‚æœ .npy æ–‡ä»¶å·²å­˜åœ¨ï¼Œåˆ™è·³è¿‡ã€‚\n",
    "    \"\"\"\n",
    "    if not os.path.exists(png_path):\n",
    "        print(f\"è­¦å‘Šï¼šå›¾åƒæ–‡ä»¶ä¸å­˜åœ¨ {png_path}\")\n",
    "        return None  # è¿”å› None ä»¥ä¾¿åç»­è¿‡æ»¤\n",
    "\n",
    "    # å°† .png æ›¿æ¢ä¸º .npy\n",
    "    npy_path = png_path.replace(\".jpg\", \".npy\")\n",
    "\n",
    "    if os.path.exists(npy_path):\n",
    "        return npy_path  # æ–‡ä»¶å·²å­˜åœ¨\n",
    "\n",
    "    try:\n",
    "        # æ‰“å¼€å›¾åƒ\n",
    "        img = Image.open(png_path)\n",
    "        \n",
    "        # Resize åˆ° 224x224\n",
    "        img_resized = img.resize((224, 224), resample=LANCZOS_RESAMPLE)\n",
    "        \n",
    "        # è½¬æ¢ä¸º NumPy æ•°ç»„\n",
    "        if img_resized.mode == 'RGBA':\n",
    "            img_resized = img_resized.convert('RGB')\n",
    "            \n",
    "        np_img = np.array(img_resized)\n",
    "        \n",
    "        # ä¿å­˜ä¸º .npy\n",
    "        np.save(npy_path, np_img)\n",
    "        \n",
    "        return npy_path\n",
    "    except Exception as e:\n",
    "        print(f\"å¤„ç†å›¾åƒå¤±è´¥ {png_path}: {e}\")\n",
    "        return None\n",
    "\n",
    "def preprocess_petfinder_flow(metadata_file, image_dir, output_dir, label_col):\n",
    "    \"\"\"\n",
    "    é¢„å¤„ç† PetFinder æ•°æ®é›†\n",
    "    \"\"\"\n",
    "    print(\"å¼€å§‹æ‰§è¡Œ PetFinder æ•°æ®é›†é¢„å¤„ç†æµç¨‹...\")\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "    # --- 1. ğŸ“– åŠ è½½æ•°æ® ---\n",
    "    print(f\"æ­£åœ¨åŠ è½½æ•°æ® {metadata_file}...\")\n",
    "    df = pd.read_csv(metadata_file)\n",
    "    print(f\"æˆåŠŸåŠ è½½ {len(df)} æ¡è®°å½•ã€‚\")\n",
    "\n",
    "    print(\"æ­£åœ¨æ„é€ å›¾åƒè·¯å¾„...\")\n",
    "    df['image_path'] = df['Id'].apply(lambda x: os.path.join(image_dir, f\"{x}.jpg\"))\n",
    "\n",
    "\n",
    "    # --- 2. ğŸ“ å®šä¹‰ç‰¹å¾ ---\n",
    "    print(\"æ­£åœ¨å®šä¹‰ç‰¹å¾...\")\n",
    "    continuous_cols = CONTINUOUS_COLS\n",
    "    categorical_cols = CATEGORICAL_COLS\n",
    "    non_feature_cols = NON_FEATURE_COLS\n",
    "    exclude_cols = [label_col, 'image_path', 'npy_path'] + non_feature_cols\n",
    "    \n",
    "    # --- 3. ğŸ§¹ æ•°æ®æ¸…æ´— ---\n",
    "    print(\"æ­£åœ¨æ¸…æ´—æ•°æ®...\")\n",
    "    \n",
    "    # è¿‡æ»¤ç¼ºå¤±å›¾åƒè·¯å¾„å’Œæ ‡ç­¾çš„è¡Œ\n",
    "    df = df.dropna(subset=[label_col])\n",
    "    print(\"æ­£åœ¨è¿›è¡Œç±»åˆ«ç‰¹å¾ç¼–ç ...\")\n",
    "    cat_dims = [] # è®°å½•æ¯ä¸ªç‰¹å¾çš„ç±»åˆ«æ•°\n",
    "    for col in categorical_cols:\n",
    "        df[col] = df[col].fillna(-1) # ç®€å•å¡«å……\n",
    "        df[col] = df[col].astype('category')\n",
    "        df[col] = df[col].cat.codes\n",
    "        # è®°å½•ç±»åˆ«æ•°é‡ (æœ€å¤§ID + 1)\n",
    "        cat_dims.append(int(df[col].max() + 1))\n",
    "\n",
    "    # --- 6. æ•°æ®é›†åˆ’åˆ† ---\n",
    "    print(\"æ­£åœ¨è¿›è¡Œ 80:10:10 æ•°æ®é›†åˆ’åˆ†...\")\n",
    "    # å›å½’ä»»åŠ¡é€šå¸¸ä¸éœ€è¦ stratifyï¼Œæˆ–è€…éœ€è¦åˆ†æ¡¶ stratifyã€‚ç®€å•èµ·è§è¿™é‡Œç”¨éšæœºåˆ’åˆ†ã€‚\n",
    "    train_df, temp_df = train_test_split(df, test_size=0.2, random_state=42)\n",
    "    val_df, test_df = train_test_split(temp_df, test_size=0.5, random_state=42)\n",
    "    \n",
    "    print(f\"åˆ’åˆ†ç»“æœ: è®­ç»ƒé›† {len(train_df)}, éªŒè¯é›† {len(val_df)}, æµ‹è¯•é›† {len(test_df)}\")\n",
    "\n",
    "    # --- 7. è¿ç»­ç‰¹å¾æ ‡å‡†åŒ– (å¦‚æœæœ‰) ---\n",
    "    if continuous_cols:\n",
    "        print(\"æ­£åœ¨æ ‡å‡†åŒ–è¿ç»­ç‰¹å¾...\")\n",
    "        scaler = StandardScaler()\n",
    "        scaler.fit(train_df[continuous_cols])\n",
    "        train_df[continuous_cols] = scaler.transform(train_df[continuous_cols])\n",
    "        val_df[continuous_cols] = scaler.transform(val_df[continuous_cols])\n",
    "        test_df[continuous_cols] = scaler.transform(test_df[continuous_cols])\n",
    "\n",
    "    # --- 8. å›¾åƒå¤„ç† ---\n",
    "    print(\"æ­£åœ¨å¤„ç†å›¾åƒ (Resize & Save .npy)...\")\n",
    "    tqdm.pandas(desc=\"Train Images\")\n",
    "    train_df['npy_path'] = train_df['image_path'].progress_apply(process_and_save_image)\n",
    "    \n",
    "    tqdm.pandas(desc=\"Val Images\")\n",
    "    val_df['npy_path'] = val_df['image_path'].progress_apply(process_and_save_image)\n",
    "    \n",
    "    tqdm.pandas(desc=\"Test Images\")\n",
    "    test_df['npy_path'] = test_df['image_path'].progress_apply(process_and_save_image)\n",
    "\n",
    "    # è¿‡æ»¤æ‰å›¾åƒå¤„ç†å¤±è´¥çš„ (è¿”å› None çš„)\n",
    "    len_before = len(train_df) + len(val_df) + len(test_df)\n",
    "    train_df = train_df.dropna(subset=['npy_path'])\n",
    "    val_df = val_df.dropna(subset=['npy_path'])\n",
    "    test_df = test_df.dropna(subset=['npy_path'])\n",
    "    len_after = len(train_df) + len(val_df) + len(test_df)\n",
    "    if len_before != len_after:\n",
    "        print(f\"è­¦å‘Šï¼šè¿‡æ»¤äº† {len_before - len_after} æ¡å›¾åƒä¸å­˜åœ¨æˆ–æŸåçš„æ•°æ®ã€‚\")\n",
    "\n",
    "    # --- 9. ä¿å­˜è¾“å‡º ---\n",
    "    print(f\"[Final] æ­£åœ¨ä¿å­˜å¤„ç†åçš„æ–‡ä»¶åˆ° {output_dir} ...\")\n",
    "\n",
    "    # ä¿å­˜å­—æ®µé•¿åº¦: å…ˆç±»åˆ«ï¼Œåè¿ç»­ (ç¬¦åˆ TIP æ¨¡å‹è¦æ±‚)\n",
    "    tabular_lengths = cat_dims + [1] * len(continuous_cols)\n",
    "    torch.save(tabular_lengths, os.path.join(output_dir, \"tabular_lengths.pt\"))\n",
    "    \n",
    "    for split_name, df_split in zip([\"train\", \"val\", \"test\"], [train_df, val_df, test_df]):\n",
    "        features_path = os.path.join(output_dir, f\"{split_name}_features.csv\")\n",
    "        # ç¡®ä¿åˆ—é¡ºåºï¼šå…ˆç±»åˆ«ï¼Œåè¿ç»­\n",
    "        cols_to_save = categorical_cols + continuous_cols\n",
    "        df_split[cols_to_save].to_csv(features_path, index=False, header=False)\n",
    "        \n",
    "        labels_path = os.path.join(output_dir, f\"{split_name}_labels.pt\")\n",
    "        # [ä¿®æ­£] æ ‡ç­¾è½¬ä¸º float32 ç”¨äºå›å½’\n",
    "        labels_tensor = torch.tensor(df_split[label_col].values, dtype=torch.float32)\n",
    "        torch.save(labels_tensor, labels_path)\n",
    "        \n",
    "        paths_path = os.path.join(output_dir, f\"{split_name}_paths.pt\")\n",
    "        # ä¿å­˜ .npy çš„ç»å¯¹è·¯å¾„\n",
    "        npy_path_list = df_split['npy_path'].tolist()\n",
    "        torch.save(npy_path_list, paths_path)\n",
    "\n",
    "    print(\"é¢„å¤„ç†å…¨éƒ¨å®Œæˆï¼\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    preprocess_petfinder_flow(\n",
    "        metadata_file=METADATA_FILE,\n",
    "        image_dir=IMAGE_DIR,\n",
    "        output_dir=OUTPUT_DIR,\n",
    "        label_col=LABEL_COL\n",
    "    )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "charms",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
